<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>SVM推导及Python实现</title>
    <url>/2022/07/27/CS229n%E4%B9%8BSVM/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Pandas常见使用</title>
    <url>/2022/08/10/Pandas%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E7%9A%84%E5%89%AF%E6%9C%AC/</url>
    <content><![CDATA[<h1 id="读取外部数据"><a href="#读取外部数据" class="headerlink" title="读取外部数据"></a>读取外部数据</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment">#读取csv，txt，excel文件</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;filename.csv&#x27;</span>)</span><br><span class="line">df = pd.read_table(<span class="string">&#x27;filename.csv&#x27;</span>)</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;filename.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>常见参数 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&#x27;filename.csv&#x27;</span>，header=<span class="literal">None</span>，sep=<span class="string">&#x27;\t&#x27;</span>,index_col=<span class="string">&quot;col1&quot;</span>,usecols=[<span class="string">&quot;col1&quot;</span>,<span class="string">&quot;col2&quot;</span>],nrows=<span class="number">5</span>，dtype=&#123;<span class="string">&quot;col1&quot;</span>:<span class="built_in">str</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>默认header&#x3D;”infer”， header&#x3D;None表示第一行不作为列名，sep指定分隔符，index_col选取某列的值作为索引，usecols仅读取需要的列,nrows为读取数据的行数，dtype设置读取数据时的字段数据类型。</p>
<h1 id="数据写入文件"><a href="#数据写入文件" class="headerlink" title="数据写入文件"></a>数据写入文件</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.to_csv(<span class="string">&#x27;filename.csv&#x27;</span>,index=<span class="literal">False</span>)</span><br><span class="line">df.to_excel(<span class="string">&#x27;filename.xlsx&#x27;</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>采用to_csv将数据写入csv文件，采用to_excel将数据写入excel文件。index&#x3D;False表示将Dataframe的索引在保存文件时去除。<br>同样也可以使用to_csv将数据保存至txt文件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.to_csv(<span class="string">&#x27;filename.txt&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Dataframe操作"><a href="#Dataframe操作" class="headerlink" title="Dataframe操作"></a>Dataframe操作</h1><p>Pandas中存储二维数据的数据结构为dataframe，下述几种dataframe类型的基本操作。</p>
<h2 id="构建dataframe"><a href="#构建dataframe" class="headerlink" title="构建dataframe"></a>构建dataframe</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]],columns=[<span class="string">&#x27;col1&#x27;</span>,<span class="string">&#x27;col2&#x27;</span>])</span><br><span class="line"></span><br><span class="line">   col1  col2</span><br><span class="line"><span class="number">0</span>     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line"><span class="number">1</span>     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line"><span class="number">2</span>     <span class="number">5</span>     <span class="number">6</span></span><br></pre></td></tr></table></figure>
<h2 id="dataframe索引"><a href="#dataframe索引" class="headerlink" title="dataframe索引"></a>dataframe索引</h2><h3 id="数据列索引"><a href="#数据列索引" class="headerlink" title="数据列索引"></a>数据列索引</h3><p>通过df[‘列名’]可以取出对应的数据列，df[[‘列名1’,’列名2]]可以取出多列的数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;col1&#x27;</span>]</span><br><span class="line"><span class="number">0</span>    <span class="number">1</span></span><br><span class="line"><span class="number">1</span>    <span class="number">3</span></span><br><span class="line"><span class="number">2</span>    <span class="number">5</span></span><br><span class="line">Name: col1, dtype: int64</span><br><span class="line"></span><br><span class="line">df[[<span class="string">&#x27;col1&#x27;</span>,<span class="string">&#x27;col2&#x27;</span>]]</span><br><span class="line">col1	col2</span><br><span class="line"><span class="number">0</span>	<span class="number">1</span>	<span class="number">2</span></span><br><span class="line"><span class="number">1</span>	<span class="number">3</span>	<span class="number">4</span></span><br><span class="line"><span class="number">2</span>	<span class="number">5</span>	<span class="number">6</span></span><br></pre></td></tr></table></figure>
<h3 id="iloc索引"><a href="#iloc索引" class="headerlink" title="iloc索引"></a>iloc索引</h3><p>iloc所以可以根据行列位置对数据进行切片，返回需要的值。iloc[ : , : ] 行列切片以“，”隔开，前面的冒号是取行数，后面的冒号是取列数(冒号切片左闭右开)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]],columns=[<span class="string">&#x27;col1&#x27;</span>,<span class="string">&#x27;col2&#x27;</span>,<span class="string">&#x27;col3&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=========================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df.iloc[<span class="number">1</span>,<span class="number">1</span>])<span class="comment">#取第二行第二列</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=========================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df.iloc[[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>]])<span class="comment">#取前两行前两列</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=========================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df.iloc[<span class="number">0</span>:<span class="number">2</span>, <span class="number">1</span>:<span class="number">2</span>])<span class="comment">#取第一至二行，第二列</span></span><br><span class="line"></span><br><span class="line">   col1  col2  col3</span><br><span class="line"><span class="number">0</span>     <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span></span><br><span class="line"><span class="number">1</span>     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span></span><br><span class="line"><span class="number">2</span>     <span class="number">5</span>     <span class="number">6</span>     <span class="number">7</span></span><br><span class="line">=========================</span><br><span class="line"><span class="number">4</span></span><br><span class="line">=========================</span><br><span class="line">   col1  col2</span><br><span class="line"><span class="number">0</span>     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line"><span class="number">1</span>     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">=========================</span><br><span class="line">   col2</span><br><span class="line"><span class="number">0</span>     <span class="number">2</span></span><br><span class="line"><span class="number">1</span>     <span class="number">4</span></span><br></pre></td></tr></table></figure>
<h2 id="dataframe分组"><a href="#dataframe分组" class="headerlink" title="dataframe分组"></a>dataframe分组</h2><p>dataframe的分组操作主要依赖groupby函数完成，基本调用方式为</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby(<span class="string">&#x27;分组维度&#x27;</span>)[<span class="string">&#x27;分组数据&#x27;</span>].操作()</span><br></pre></td></tr></table></figure>
<p>主要应用于依据某列分组，统计待分组数据在该列维度下的数据值，如对于房价数据，根据<em>城市</em>分组，统计该在城市的<em>房屋价格</em>最大值。以data2.csv数据集为例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df=pd.read_csv(<span class="string">&#x27;data2.csv&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=========================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df.groupby(<span class="string">&#x27;name&#x27;</span>)[<span class="string">&#x27;C&#x27;</span>].<span class="built_in">max</span>())<span class="comment">#按name列分组对应的C列的最大值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=========================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df.groupby(<span class="string">&#x27;name&#x27;</span>)[<span class="string">&#x27;B&#x27;</span>].mean())<span class="comment">#按name列分组对应的C列的平均值</span></span><br><span class="line"></span><br><span class="line">   A     B   C    D      name</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80</span>  NaN        HK</span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75</span> -<span class="number">1.0</span>  Shanghai</span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   <span class="number">7.0</span>  <span class="number">75</span>  <span class="number">0.0</span>        HK</span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>  <span class="number">80</span> -<span class="number">1.0</span>   Beijing</span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80</span>  <span class="number">1.0</span>   Beijing</span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>  <span class="number">75</span>  NaN     Macau</span><br><span class="line">=========================</span><br><span class="line">name</span><br><span class="line">Beijing     <span class="number">80</span></span><br><span class="line">HK          <span class="number">80</span></span><br><span class="line">Macau       <span class="number">75</span></span><br><span class="line">Shanghai    <span class="number">75</span></span><br><span class="line">Name: C, dtype: int64</span><br><span class="line">=========================</span><br><span class="line">name</span><br><span class="line">Beijing     <span class="number">6.45</span></span><br><span class="line">HK          <span class="number">8.75</span></span><br><span class="line">Macau       <span class="number">8.30</span></span><br><span class="line">Shanghai    <span class="number">5.60</span></span><br><span class="line">Name: B, dtype: float64</span><br></pre></td></tr></table></figure>
<h1 id="array和dataframe相互转换"><a href="#array和dataframe相互转换" class="headerlink" title="array和dataframe相互转换"></a>array和dataframe相互转换</h1><p>array转换为dataframe，pd.DataFrame将array类型数据转换为DataFrame，默认列名为0，1…，参数columns设置列名。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = pd.DataFrame(df.values,columns=<span class="built_in">list</span>(<span class="string">&quot;AB&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line">   A  B</span><br><span class="line"><span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span></span><br><span class="line"><span class="number">1</span>  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line"><span class="number">2</span>  <span class="number">5</span>  <span class="number">6</span></span><br></pre></td></tr></table></figure>
<p>dataframe转换为array，values将dataframe格式数据转换为array对象。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.values</span><br><span class="line"></span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<h1 id="数据特征统计函数"><a href="#数据特征统计函数" class="headerlink" title="数据特征统计函数"></a>数据特征统计函数</h1><p>info()返回数据表的列，缺失值情况，数据类型等信息，以数据表HousingData.csv为例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&quot;HousingData.csv&quot;</span>)</span><br><span class="line">df.info()</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;pandas.core.frame.DataFrame&#x27;</span>&gt;</span><br><span class="line">RangeIndex: <span class="number">506</span> entries, <span class="number">0</span> to <span class="number">505</span></span><br><span class="line">Data columns (total <span class="number">14</span> columns):</span><br><span class="line"> <span class="comment">#   Column   Non-Null Count  Dtype  </span></span><br><span class="line">---  ------   --------------  -----  </span><br><span class="line"> <span class="number">0</span>   CRIM     <span class="number">486</span> non-null    float64</span><br><span class="line"> <span class="number">1</span>   ZN       <span class="number">486</span> non-null    float64</span><br><span class="line"> <span class="number">2</span>   INDUS    <span class="number">486</span> non-null    float64</span><br><span class="line"> <span class="number">3</span>   CHAS     <span class="number">486</span> non-null    float64</span><br><span class="line"> <span class="number">4</span>   NOX      <span class="number">506</span> non-null    float64</span><br><span class="line">dtypes: float64(<span class="number">12</span>), int64(<span class="number">2</span>)</span><br><span class="line">memory usage: <span class="number">55.5</span> KB</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>df.describe()返回数据表中每列的的计数，平均值，最大最小值，分位数等统计量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.describe()</span><br><span class="line"></span><br><span class="line">             CRIM          ZN       INDUS        CHAS         NOX</span><br><span class="line">count  <span class="number">486.000000</span>  <span class="number">486.000000</span>  <span class="number">486.000000</span>  <span class="number">486.000000</span>  <span class="number">506.000000</span></span><br><span class="line">mean     <span class="number">3.611874</span>   <span class="number">11.211934</span>   <span class="number">11.083992</span>    <span class="number">0.069959</span>    <span class="number">0.554695</span></span><br><span class="line">std      <span class="number">8.720192</span>   <span class="number">23.388876</span>    <span class="number">6.835896</span>    <span class="number">0.255340</span>    <span class="number">0.115878</span></span><br><span class="line"><span class="built_in">min</span>      <span class="number">0.006320</span>    <span class="number">0.000000</span>    <span class="number">0.460000</span>    <span class="number">0.000000</span>    <span class="number">0.385000</span></span><br><span class="line"><span class="number">25</span>%      <span class="number">0.081900</span>    <span class="number">0.000000</span>    <span class="number">5.190000</span>    <span class="number">0.000000</span>    <span class="number">0.449000</span></span><br><span class="line"><span class="number">50</span>%      <span class="number">0.253715</span>    <span class="number">0.000000</span>    <span class="number">9.690000</span>    <span class="number">0.000000</span>    <span class="number">0.538000</span></span><br><span class="line"><span class="number">75</span>%      <span class="number">3.560263</span>   <span class="number">12.500000</span>   <span class="number">18.100000</span>    <span class="number">0.000000</span>    <span class="number">0.624000</span></span><br><span class="line"><span class="built_in">max</span>     <span class="number">88.976200</span>  <span class="number">100.000000</span>   <span class="number">27.740000</span>    <span class="number">1.000000</span>    <span class="number">0.871000</span></span><br></pre></td></tr></table></figure>
<h1 id="唯一值函数"><a href="#唯一值函数" class="headerlink" title="唯一值函数"></a>唯一值函数</h1><p>value_counts()可以统计指定列中的值以及值出现的次数。在下面的例子中统计“name”列中出现的值以及出现值的次数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = pd.read_csv(<span class="string">&quot;data2.csv&quot;</span>)</span><br><span class="line">df2[<span class="string">&#x27;name&#x27;</span>].value_counts()</span><br><span class="line"></span><br><span class="line">HK          <span class="number">2</span></span><br><span class="line">Beijing     <span class="number">2</span></span><br><span class="line">Shanghai    <span class="number">1</span></span><br><span class="line">Macau       <span class="number">1</span></span><br><span class="line">Name: name, dtype: int64</span><br></pre></td></tr></table></figure>
<h1 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h1><h2 id="查找缺失值"><a href="#查找缺失值" class="headerlink" title="查找缺失值"></a>查找缺失值</h2><p>isnull()判断数据每一列中是否有空值，有则返回True，没有则返回False。isnull().sum()统计每一列中的空值数量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.isnull().<span class="built_in">sum</span>()</span><br><span class="line">CRIM       <span class="number">20</span></span><br><span class="line">ZN         <span class="number">20</span></span><br><span class="line">INDUS      <span class="number">20</span></span><br><span class="line">CHAS       <span class="number">20</span></span><br><span class="line">NOX         <span class="number">0</span></span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure>
<h2 id="删除nan值并重置序号"><a href="#删除nan值并重置序号" class="headerlink" title="删除nan值并重置序号"></a>删除nan值并重置序号</h2><p>以文件data.csv为例，展示以下几种缺失值处理方法。读取data.csv可见有多处缺失值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1 = pd.read_csv(<span class="string">&quot;data.csv&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df1)</span><br><span class="line">A     B     C    D</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80.0</span>  NaN</span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75.0</span> -<span class="number">1.0</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   NaN  <span class="number">75.0</span>  <span class="number">0.0</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>   NaN -<span class="number">1.0</span></span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80.0</span>  <span class="number">1.0</span></span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>   NaN  NaN</span><br></pre></td></tr></table></figure>
<p>dropna()删除所有含空值的行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1.dropna()</span><br><span class="line"></span><br><span class="line">A	B	C	D</span><br><span class="line"><span class="number">1</span>	<span class="number">4</span>	<span class="number">5.6</span>	<span class="number">75.0</span>	-<span class="number">1.0</span></span><br><span class="line"><span class="number">4</span>	<span class="number">6</span>	<span class="number">3.5</span>	<span class="number">80.0</span>	<span class="number">1.0</span></span><br></pre></td></tr></table></figure>
<p>reset_index()对删除空值后的行重置序号,参数drop&#x3D;True删除原来的index。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	A	B	C	D</span><br><span class="line"><span class="number">0</span>	<span class="number">4</span>	<span class="number">5.6</span>	<span class="number">75.0</span>	-<span class="number">1.0</span></span><br><span class="line"><span class="number">1</span>	<span class="number">6</span>	<span class="number">3.5</span>	<span class="number">80.0</span>	<span class="number">1.0</span></span><br></pre></td></tr></table></figure>
<h2 id="缺失值填充"><a href="#缺失值填充" class="headerlink" title="缺失值填充"></a>缺失值填充</h2><h3 id="均值填充"><a href="#均值填充" class="headerlink" title="均值填充"></a>均值填充</h3><p>采用df.replace或fillna()将缺失值处替换为均值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#采用replace将D列的缺失值替换为平均值</span></span><br><span class="line">df1.replace(&#123;<span class="string">&#x27;D&#x27;</span>:np.nan&#125;,np.mean(df1[<span class="string">&#x27;D&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">   A     B     C     D</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80.0</span> -<span class="number">0.25</span></span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75.0</span> -<span class="number">1.00</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   NaN  <span class="number">75.0</span>  <span class="number">0.00</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>   NaN -<span class="number">1.00</span></span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80.0</span>  <span class="number">1.00</span></span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>   NaN -<span class="number">0.25</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#采用fillna将D列的缺失值替换为平均值</span></span><br><span class="line">df1[<span class="string">&#x27;D&#x27;</span>].fillna(df1[<span class="string">&#x27;D&#x27;</span>].mean(),inplace=<span class="literal">True</span>)</span><br><span class="line">   A     B     C     D</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80.0</span> -<span class="number">0.25</span></span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75.0</span> -<span class="number">1.00</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   NaN  <span class="number">75.0</span>  <span class="number">0.00</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>   NaN -<span class="number">1.00</span></span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80.0</span>  <span class="number">1.00</span></span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>   NaN -<span class="number">0.25</span></span><br></pre></td></tr></table></figure>
<h3 id="其他值填充"><a href="#其他值填充" class="headerlink" title="其他值填充"></a>其他值填充</h3><p>用一组数据中出现最多次数的值填充，scipy.stats.mode()统计一组数据中出现次数最多的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> st</span><br><span class="line">mode_val=st.mode(df1[<span class="string">&#x27;D&#x27;</span>])[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">df1.replace(&#123;<span class="string">&#x27;D&#x27;</span>:np.nan&#125;,mode_val)</span><br><span class="line"></span><br><span class="line">   A     B     C    D</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80.0</span> -<span class="number">1.0</span></span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75.0</span> -<span class="number">1.0</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   NaN  <span class="number">75.0</span>  <span class="number">0.0</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>   NaN -<span class="number">1.0</span></span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80.0</span>  <span class="number">1.0</span></span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>   NaN -<span class="number">1.0</span></span><br></pre></td></tr></table></figure>
<h3 id="回归预测填充"><a href="#回归预测填充" class="headerlink" title="回归预测填充"></a>回归预测填充</h3><p>对于缺失值为连续性数据，数据之间存在依赖关系的值，可以采用根据其他列的数据，带入线性回归等有监督学习方法来估计缺失值的方法进行缺失值填充。示例采用线性回归模型对D列的缺失值进行填充。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = pd.read_csv(<span class="string">&quot;data2.csv&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df2)</span><br><span class="line"></span><br><span class="line">   A     B   C    D      name</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80</span>  NaN        HK</span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75</span> -<span class="number">1.0</span>  Shanghai</span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   <span class="number">7.0</span>  <span class="number">75</span>  <span class="number">0.0</span>        HK</span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>  <span class="number">80</span> -<span class="number">1.0</span>   Beijing</span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80</span>  <span class="number">1.0</span>   Beijing</span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>  <span class="number">75</span>  NaN     Macau</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model<span class="comment">#导入线性回归工具包</span></span><br><span class="line">df_train = df2.dropna()</span><br><span class="line">x = df_train.drop(columns=[<span class="string">&#x27;D&#x27;</span>,<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line">reg = linear_model.LinearRegression()<span class="comment">#训练拟合模型</span></span><br><span class="line">reg.fit(x,df_train[<span class="string">&#x27;D&#x27;</span>])</span><br><span class="line">predict_d = reg.predict(df2.drop(columns=[<span class="string">&#x27;D&#x27;</span>,<span class="string">&#x27;name&#x27;</span>]))<span class="comment">#预测新的D列</span></span><br><span class="line">df2[<span class="string">&#x27;D&#x27;</span>] = predict_d<span class="comment">#回归预测填充后的数据</span></span><br><span class="line"></span><br><span class="line">   A     B   C             D      name</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80</span>  <span class="number">3.193548e+00</span>        HK</span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75</span> -<span class="number">1.000000e+00</span>  Shanghai</span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   <span class="number">7.0</span>  <span class="number">75</span> -<span class="number">7.105427e-15</span>        HK</span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>  <span class="number">80</span> -<span class="number">1.000000e+00</span>   Beijing</span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80</span>  <span class="number">1.000000e+00</span>   Beijing</span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>  <span class="number">75</span>  <span class="number">1.129032e+00</span>     Macau</span><br></pre></td></tr></table></figure>
<h3 id="hotdeck填充"><a href="#hotdeck填充" class="headerlink" title="hotdeck填充"></a>hotdeck填充</h3><p>将缺失值替换为在一列中随机选择的值。采用ffill方法，将空值填补为上一行中出现的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.fillna(method=<span class="string">&#x27;ffill&#x27;</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h1 id="分类标签数值化"><a href="#分类标签数值化" class="headerlink" title="分类标签数值化"></a>分类标签数值化</h1><h2 id="序数编码"><a href="#序数编码" class="headerlink" title="序数编码"></a>序数编码</h2><p>采用cat.codes将非数值标签进行简单序数编码，例如将Beijing编码为0，HK编码为1，Macau编码为2.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.name = pd.Categorical(df[<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line">df[<span class="string">&#x27;code&#x27;</span>] = df.name.cat.codes</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"></span><br><span class="line">   A     B   C    D      name  code</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80</span>  NaN        HK     <span class="number">1</span></span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75</span> -<span class="number">1.0</span>  Shanghai     <span class="number">3</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   <span class="number">7.0</span>  <span class="number">75</span>  <span class="number">0.0</span>        HK     <span class="number">1</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>  <span class="number">80</span> -<span class="number">1.0</span>   Beijing     <span class="number">0</span></span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80</span>  <span class="number">1.0</span>   Beijing     <span class="number">0</span></span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>  <span class="number">75</span>  <span class="number">1.0</span>     Macau     <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h2><p>one-hot编码将每一个分类类别编码为唯一的二进制向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.get_dummies(df.name)</span><br><span class="line"></span><br><span class="line">   Beijing  HK  Macau  Shanghai</span><br><span class="line"><span class="number">0</span>        <span class="number">0</span>   <span class="number">1</span>      <span class="number">0</span>         <span class="number">0</span></span><br><span class="line"><span class="number">1</span>        <span class="number">0</span>   <span class="number">0</span>      <span class="number">0</span>         <span class="number">1</span></span><br><span class="line"><span class="number">2</span>        <span class="number">0</span>   <span class="number">1</span>      <span class="number">0</span>         <span class="number">0</span></span><br><span class="line"><span class="number">3</span>        <span class="number">1</span>   <span class="number">0</span>      <span class="number">0</span>         <span class="number">0</span></span><br><span class="line"><span class="number">4</span>        <span class="number">1</span>   <span class="number">0</span>      <span class="number">0</span>         <span class="number">0</span></span><br><span class="line"><span class="number">5</span>        <span class="number">0</span>   <span class="number">0</span>      <span class="number">1</span>         <span class="number">0</span></span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title>在云服务器上安装hadoop</title>
    <url>/2023/01/20/Untitled%201/</url>
    <content><![CDATA[<h2 id="Secure-Virtual-Machine-Setup"><a href="#Secure-Virtual-Machine-Setup" class="headerlink" title="Secure Virtual Machine Setup"></a>Secure Virtual Machine Setup</h2><h3 id="set-up-a-virtual-machine"><a href="#set-up-a-virtual-machine" class="headerlink" title="set up a virtual machine"></a>set up a virtual machine</h3><p>Use google cloud-&gt;compute engine-&gt;virtual machine instance set up a virtual</p>
<p>Machine info: n2-standard-2 Intel Cascade Lake 100GB</p>
<p>System: Ubuntu-18.04 </p>
<p>![截屏2023-01-20 下午10.26.13](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Documents&#x2F;截屏2023-01-20 下午10.26.13.png)</p>
<h3 id="set-up-a-firewall"><a href="#set-up-a-firewall" class="headerlink" title="set up a firewall"></a>set up a firewall</h3><p>Use VPC-&gt;firevall set a firewall set the source ip to ‘137.189.0.0&#x2F;16’, that only CUHK can access to the vm.</p>
<p>![截屏2023-01-20 下午10.35.29](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-20 下午10.35.29.png)</p>
<h3 id="set-up-a-SSH-1"><a href="#set-up-a-SSH-1" class="headerlink" title="set up a SSH^[1]^"></a>set up a SSH^[1]^</h3><p>Generate a key pair at local machine</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -f id_rsa</span><br></pre></td></tr></table></figure>

<p>upload the public-key to the VM, and use the secret key to access to the VM.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">shiyuzhuo@LoBP key % ssh -i id_rsa shiyuzhuo@34.92.6.155 </span><br></pre></td></tr></table></figure>

<h1 id="Hadoop-Cluster-Setup"><a href="#Hadoop-Cluster-Setup" class="headerlink" title="Hadoop Cluster Setup"></a>Hadoop Cluster Setup</h1><h2 id="Single-node-Hadoop-Setup"><a href="#Single-node-Hadoop-Setup" class="headerlink" title="Single-node Hadoop Setup"></a>Single-node Hadoop Setup</h2><h3 id="add-a-Hadoop-user"><a href="#add-a-Hadoop-user" class="headerlink" title="add a Hadoop user"></a>add a Hadoop user</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">shiyuzhuo@singlenode:~$ sudo useradd -m hadoop -s /bin/bash</span><br><span class="line">shiyuzhuo@singlenode:~$ sudo passwd hadoop</span><br><span class="line">shiyuzhuo@singlenode:~$ sudo adduser hadoop sudo</span><br><span class="line">shiyuzhuo@singlenode:~$ su hadoop</span><br></pre></td></tr></table></figure>

<h3 id="insatll-SSH"><a href="#insatll-SSH" class="headerlink" title="insatll SSH"></a>insatll SSH</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:~$ sudo apt-get install openssh-server</span><br><span class="line">hadoop@singlenode:~$ ssh localhost</span><br><span class="line">hadoop@singlenode:~$ cd ~/.ssh/ </span><br><span class="line">hadoop@singlenode:~/.ssh$ ssh-keygen -t rsa</span><br><span class="line">hadoop@singlenode:~/.ssh$ cat ./id_rsa.pub &gt;&gt; ./authorized_keys</span><br><span class="line">hadoop@singlenode:~/.ssh$ cd ~</span><br><span class="line">hadoop@singlenode:~$ ssh localhost</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="insatll-JAVA"><a href="#insatll-JAVA" class="headerlink" title="insatll JAVA"></a>insatll JAVA</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:~$ sudo apt-get update</span><br><span class="line">hadoop@singlenode:~$ sudo apt-get install openjdk-8-jre</span><br><span class="line">hadoop@singlenode:~$ sudo apt-get install openjdk-8-jdk</span><br><span class="line">hadoop@singlenode:~$ vim ~/.bashrc</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line">hadoop@singlenode:~$ source  ~/.bashrc</span><br><span class="line">hadoop@singlenode:~$ java -version</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="install-hadoop"><a href="#install-hadoop" class="headerlink" title="install hadoop"></a>install hadoop</h3><p>Download Hadoop and give the user Hadoop permission</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:~$ wget https://archive.apache.org/dist/hadoop/core/hadoop-2.9.2/hadoop-2.9.2.tar.gz</span><br><span class="line">hadoop@singlenode:~$ sudo tar -zxf hadoop-2.9.2.tar.gz -C /usr/local</span><br><span class="line">hadoop@singlenode:~$ cd /usr/local</span><br><span class="line">hadoop@singlenode:/usr/local$ sudo mv ./hadoop-2.9.2 ./hadoop</span><br><span class="line">hadoop@singlenode:/usr/local$ sudo chown -R hadoop ./hadoop</span><br></pre></td></tr></table></figure>

<p>add Java path and launch the Hadoop service</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:/usr/local$ cd hadoop</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ sudo vi etc/hadoop/hadoop-env.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">set</span> to the root of your Java installation</span></span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ bin/hadoop</span><br></pre></td></tr></table></figure>

<p>set up a single-node Hadoop cluster in a pseudo-distributed mode,change the configuration^[3]^</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:/usr/local/hadoop$ sudo vi ./etc/hadoop/core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ sudo vi ./etc/hadoop/hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>configure the environment</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:~$ sudo vi /home/hadoop/.bashrc</span><br><span class="line">export HADOOP_PREFIX=/usr/local/hadoop</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_COMMON_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_HDFS_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export YARN_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$&#123;HADOOP_PREFIX&#125;/lib/native</span><br><span class="line">export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_PREFIX/lib&quot;</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line">export JRE_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre</span><br><span class="line">export PATH=$PATH:/usr/lib/jvm/java-8-openjdk-amd64/bin:/usr/lib/jvm/java-8-openjdk-amd64/jre/bin</span><br></pre></td></tr></table></figure>

<p>Start the service</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:~$ cd /usr/local/hadoop</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ ./bin/hdfs namenode -format</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ ./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>use jps check the single node statement</p>
<h3 id="截屏2023-01-21-上午2-28-19-x2F-Users-x2F-shiyuzhuo-x2F-Library-x2F-Application-Support-x2F-typora-user-images-x2F-截屏2023-01-21-上午2-28-19-png"><a href="#截屏2023-01-21-上午2-28-19-x2F-Users-x2F-shiyuzhuo-x2F-Library-x2F-Application-Support-x2F-typora-user-images-x2F-截屏2023-01-21-上午2-28-19-png" class="headerlink" title="![截屏2023-01-21 上午2.28.19](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午2.28.19.png)"></a>![截屏2023-01-21 上午2.28.19](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午2.28.19.png)</h3><p>visit 34.92.6.155:50070 to access to the hadoop webpage</p>
<p>![3481674134925_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3481674134925_.pic_hd.jpg)</p>
<p>![3491674134925_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3491674134925_.pic_hd.jpg)</p>
<p>The signle node hadoop setup successfully</p>
<h3 id="run-the-Terasort-example-4"><a href="#run-the-Terasort-example-4" class="headerlink" title="run the Terasort example^[4]^"></a>run the Terasort example^[4]^</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:/usr/local/hadoop$  ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar teragen 100000 terasort/input</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar terasort terasort/input terasort/output</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar teravalidate terasort/output terasort/check</span><br></pre></td></tr></table></figure>

<p>![3501674135120_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3501674135120_.pic_hd.jpg)</p>
<p>![3511674135154_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3511674135154_.pic_hd.jpg)</p>
<p>![3521674135172_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3521674135172_.pic_hd.jpg)</p>
<p>![3541674135210_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3541674135210_.pic_hd.jpg)</p>
<h2 id="Multi-node-Hadoop-Cluster-Setup"><a href="#Multi-node-Hadoop-Cluster-Setup" class="headerlink" title="Multi-node Hadoop Cluster Setup"></a>Multi-node Hadoop Cluster Setup</h2><h3 id="set-up-4-instance-on-the-cloud-VM"><a href="#set-up-4-instance-on-the-cloud-VM" class="headerlink" title="set up 4 instance on the cloud VM"></a>set up 4 instance on the cloud VM</h3><p>One is master and the other are slave1,slave2 and slave3. All of them have the same settings</p>
<h2 id="截屏2023-01-21-上午2-40-56-x2F-Users-x2F-shiyuzhuo-x2F-Library-x2F-Application-Support-x2F-typora-user-images-x2F-截屏2023-01-21-上午2-40-56-png"><a href="#截屏2023-01-21-上午2-40-56-x2F-Users-x2F-shiyuzhuo-x2F-Library-x2F-Application-Support-x2F-typora-user-images-x2F-截屏2023-01-21-上午2-40-56-png" class="headerlink" title="![截屏2023-01-21 上午2.40.56](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午2.40.56.png)"></a>![截屏2023-01-21 上午2.40.56](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午2.40.56.png)</h2><p>Set up a hadoop user and modify the SSH configure</p>
<figure class="highlight shell"><figcaption><span>l</span></figcaption><table><tr><td class="code"><pre><span class="line">shiyuzhuo@master:~$ sudo -i</span><br><span class="line">root@master:~# adduser hadoop</span><br><span class="line">root@master:~# vi /etc/ssh/sshd_config</span><br><span class="line">root@master:~# service sshd restart</span><br></pre></td></tr></table></figure>

<p>change the PasswordAuthentication yes</p>
<p>and the ChallengResponseAuthentication yes</p>
<h3 id="set-up-SSH-in-the-cluster"><a href="#set-up-SSH-in-the-cluster" class="headerlink" title="set up SSH in the cluster"></a>set up SSH in the cluster</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@master:~# sudo vi /etc/hosts</span><br><span class="line">10.170.0.2 master.asia-east2-a.c.multi-node2.internal master  </span><br><span class="line">10.170.0.3 slave1.asia-east2-a.c.multi-node2.internal slave1</span><br><span class="line">10.170.0.4 slave2.asia-east2-a.c.multi-node2.internal slave2</span><br><span class="line">10.170.0.5 slave3.asia-east2-a.c.multi-node2.internal slave3</span><br></pre></td></tr></table></figure>

<p>repeat the above commad on the master and 3 slaves.Then generate key pair</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ ssh-keygen -t rsa -P &quot;&quot;</span><br><span class="line">hadoop@master:~$ cat ./.ssh/id_rsa.pub &gt;&gt; ./authorized_keys</span><br><span class="line">hadoop@master:~$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@master</span><br><span class="line">hadoop@master:~$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@slave1</span><br><span class="line">hadoop@master:~$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@slave2</span><br><span class="line">hadoop@master:~$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@slave3</span><br><span class="line">chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<p>test if master can use ssh to connect slaves</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ ssh slave1</span><br></pre></td></tr></table></figure>

<p>![截屏2023-01-21 上午3.17.43](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午3.17.43.png)</p>
<p>successfully set up connections</p>
<h3 id="install-java-and-hadoop-on-the-master"><a href="#install-java-and-hadoop-on-the-master" class="headerlink" title="install java and hadoop on the master"></a>install java and hadoop on the master</h3><p>Just as the steps in the Q0, install java and hadoop and configure the environment.</p>
<h3 id="configure-hadoop"><a href="#configure-hadoop" class="headerlink" title="configure hadoop"></a>configure hadoop</h3><p>New the flod of data node and namenode</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ sudo mkdir -p /usr/local/hadoop_store/tmp </span><br><span class="line">hadoop@master:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode </span><br><span class="line">hadoop@master:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode </span><br><span class="line">hadoop@master:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/secondarynamenode </span><br></pre></td></tr></table></figure>

<p>configure the hadoop documents</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:/usr/local/hadoop/etc/hadoop$ vi hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    	&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    	&lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop_store/hdfs/namenode&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop_store/hdfs/datanode&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop_store/hdfs/secondarynamenode&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3600&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">hadoop@master:/usr/local/hadoop/etc/hadoop$ vi core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/usr/local/hadoop_store/tmp&lt;/value&gt;</span><br><span class="line">    	&lt;description&gt;A base for other temporary directories.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://master:54310&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">hadoop@master:/usr/local/hadoop/etc/hadoop$ vi mapred-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapred.job.tracker&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:54311&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">hadoop@master:/usr/local/hadoop/etc/hadoop$ vi yarn-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">hadoop@slave1:~$ chown -R hadoop:hadoop /usr/local/hadoop</span><br><span class="line">hadoop@slave1:~$ chown -R hadoop:hadoop /usr/local/hadoop_store</span><br></pre></td></tr></table></figure>

<p>Add the slves to the slave file</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ sudo vi /usr/local/hadoop/etc/hadoop/slaves</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">localhost</span></span><br><span class="line">master</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br><span class="line">slave3</span><br></pre></td></tr></table></figure>

<h3 id="copy-the-document-from-master-to-slaves"><a href="#copy-the-document-from-master-to-slaves" class="headerlink" title="copy the document from master to slaves"></a>copy the document from master to slaves</h3><p>Use SCP copy the hadoop settings to slaves. Firstly install java on the three slaves as the steps in Q0, the copy the hadoop stein’s to slaves</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ scp -r /usr/local/hadoop slave1:/home/hadoop/ </span><br><span class="line">hadoop@master:~$ scp -r /usr/local/hadoop_store slave1:/home/hadoop/</span><br><span class="line">hadoop@master:~$ scp -r /home/hadoop/.bashrc slave1:/home/hadoop/</span><br></pre></td></tr></table></figure>

<p>Repeat the commad 3 times, copy the hadoop to the 3 slaves. Then change the catalog and permission of hadoop on each slaves</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@slave3:~$ sudo mv /home/hadoop/hadoop /usr/local/</span><br><span class="line">hadoop@slave3:~$ sudo mv /home/hadoop/hadoop_store /usr/local/</span><br><span class="line">hadoop@slave3:~$ chown -R hadoop:hadoop /usr/local/hadoop</span><br><span class="line">hadoop@slave3:~$ chown -R hadoop:hadoop /usr/local/hadoop_store</span><br></pre></td></tr></table></figure>

<h3 id="Start-the-multi-node-hadoop-cluster"><a href="#Start-the-multi-node-hadoop-cluster" class="headerlink" title="Start the multi-node hadoop cluster"></a>Start the multi-node hadoop cluster</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ hadoop namenode -format</span><br><span class="line">hadoop@master:~$ start-dfs.sh</span><br><span class="line">hadoop@master:~$ start-yarn.sh</span><br></pre></td></tr></table></figure>

<p>use ups check the status</p>
<p>on master</p>
<p>![截屏2023-01-21 上午3.43.06](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午3.43.06.png)</p>
<p>On slaves</p>
<p>![截屏2023-01-21 上午3.43.35](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午3.43.35.png)</p>
<p>visit the web page</p>
<p>![3591674217838_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3591674217838_.pic_hd.jpg)</p>
<p>Successfully start the service.</p>
<h3 id="generate-2-different-datasets-of-size-2GB-and-20GB-to-serve-as-input-for-the-Terasort-program-Run-the-Terasort-code-again-for-these-different-datasets-and-compare-their-running-time"><a href="#generate-2-different-datasets-of-size-2GB-and-20GB-to-serve-as-input-for-the-Terasort-program-Run-the-Terasort-code-again-for-these-different-datasets-and-compare-their-running-time" class="headerlink" title="generate 2 different datasets of size 2GB and 20GB to serve as input for the Terasort program. Run the Terasort code again for these different datasets and compare their running time."></a>generate 2 different datasets of size 2GB and 20GB to serve as input for the Terasort program. Run the Terasort code again for these different datasets and compare their running time.</h3><p>Just like the steps in a, firstly run the 2GB dataset</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar teragen 20000000 terasort/input2GB </span><br><span class="line">hadoop@master:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar terasort terasort/input2GB terasort/output2GB </span><br><span class="line">hadoop@master:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar teravalidate terasort/output2GB terasort/check2GB </span><br></pre></td></tr></table></figure>

<p>Run the 20GB</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar teragen 200000000 terasort/input20GB </span><br><span class="line">hadoop@master:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar terasort terasort/input2GB terasort/output20GB </span><br><span class="line">hadoop@master:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar teravalidate terasort/output2GB terasort/check20GB </span><br></pre></td></tr></table></figure>

<p>visit the 34.96.212.237:8088 to monitor the runtime</p>
<p>![截屏2023-01-21 上午3.52.47](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Documents&#x2F;截屏2023-01-21 上午3.52.47.png)</p>
<p>the 2GB runtime: 131 seconds</p>
<p>the 20GB runtime:13755 seconds</p>
<h2 id="Running-the-Python-Code-on-Hadoop"><a href="#Running-the-Python-Code-on-Hadoop" class="headerlink" title="Running the Python Code on Hadoop"></a>Running the Python Code on Hadoop</h2><h3 id="new-the-mapper-py-and-the-reducer-py"><a href="#new-the-mapper-py-and-the-reducer-py" class="headerlink" title="new the mapper.py and the reducer.py"></a>new the mapper.py and the reducer.py</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;mapper.py&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># input comes from STDIN (standard input)</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    <span class="comment"># remove leading and trailing whitespace</span></span><br><span class="line">    line = line.strip()</span><br><span class="line">    <span class="comment"># split the line into words</span></span><br><span class="line">    words = line.split()</span><br><span class="line">    <span class="comment"># increase counters</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="comment"># write the results to STDOUT (standard output);</span></span><br><span class="line">        <span class="comment"># what we output here will be the input for the</span></span><br><span class="line">        <span class="comment"># Reduce step, i.e. the input for reducer.py</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># tab-delimited; the trivial word count is 1</span></span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;%s\t%s&#x27;</span> % (word, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;reducer.py&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">current_word = <span class="literal">None</span></span><br><span class="line">current_count = <span class="number">0</span></span><br><span class="line">word = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># input comes from STDIN</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    <span class="comment"># remove leading and trailing whitespace</span></span><br><span class="line">    line = line.strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse the input we got from mapper.py</span></span><br><span class="line">    word, count = line.split(<span class="string">&#x27;\t&#x27;</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert count (currently a string) to int</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        count = <span class="built_in">int</span>(count)</span><br><span class="line">    <span class="keyword">except</span> ValueError:</span><br><span class="line">        <span class="comment"># count was not a number, so silently</span></span><br><span class="line">        <span class="comment"># ignore/discard this line</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># this IF-switch only works because Hadoop sorts map output</span></span><br><span class="line">    <span class="comment"># by key (here: word) before it is passed to the reducer</span></span><br><span class="line">    <span class="keyword">if</span> current_word == word:</span><br><span class="line">        current_count += count</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> current_word:</span><br><span class="line">            <span class="comment"># write result to STDOUT</span></span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&#x27;%s\t%s&#x27;</span> % (current_word, current_count))</span><br><span class="line">        current_count = count</span><br><span class="line">        current_word = word</span><br><span class="line"></span><br><span class="line"><span class="comment"># do not forget to output the last word if needed!</span></span><br><span class="line"><span class="keyword">if</span> current_word == word:</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&#x27;%s\t%s&#x27;</span> % (current_word, current_count))</span><br></pre></td></tr></table></figure>

<h3 id="download-the-dataset-and-upload-the-dataset-to-the-VM"><a href="#download-the-dataset-and-upload-the-dataset-to-the-VM" class="headerlink" title="download the dataset and upload the dataset to the VM"></a>download the dataset and upload the dataset to the VM</h3><p>download the shakespeare dataset, then upload</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">shiyuzhuo@LoBP key3 % scp -i id_rsa /Users/shiyuzhuo/Documents/shakespeare hadoop@34.96.212.237:~</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>upload the data to HDFS</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ hadoop dfs -copyFromLocal /home/hadoop/shakespeare /user/hadoop/shakespeare</span><br></pre></td></tr></table></figure>

<p>start the map reduce </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar -file /home/hduser/mapper.py -mapper /home/hduser/mapper.py -file /home/hduser/reducer.py -reducer /home/hduser/reducer.py -input /user/hduser/shakespeare -output /user/hduser/shakespeare-output</span><br></pre></td></tr></table></figure>

<p>Check the yarn page</p>
<p>![3651674228851_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3651674228851_.pic_hd.jpg)</p>
<p>successfully run the map reduce job</p>
<p>![截屏2023-01-21 上午4.07.03](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午4.07.03.png)</p>
<p>download the result from HDFS to local to see the output result data</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~/shakespeare-output$ hadoop dfs -copyToLocal /user/hadoop/shakespeare-output3 /home/hadoop</span><br><span class="line">hadoop@master:~/shakespeare-output3$ vi part-00000</span><br></pre></td></tr></table></figure>

<p>The map reduce result</p>
<p>![3631674228731_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3631674228731_.pic_hd.jpg)</p>
<p>Total running time is 54240(ms)</p>
<h2 id="Compiling-the-Java-WordCount-program-for-MapReduce"><a href="#Compiling-the-Java-WordCount-program-for-MapReduce" class="headerlink" title="Compiling the Java WordCount program for MapReduce"></a>Compiling the Java WordCount program for MapReduce</h2><h3 id="use-“hadoop-classpath”-command-to-get-all-Hadoop-jars"><a href="#use-“hadoop-classpath”-command-to-get-all-Hadoop-jars" class="headerlink" title="use “hadoop classpath” command to get all Hadoop jars."></a>use “hadoop classpath” command to get all Hadoop jars.</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ echo &#x27;export HADOOP_CLASSPATH=$&#123;JAVA_HOME&#125;/lib/tools.jar&#x27; &gt;&gt; /home/hadoop/.bashrc</span><br><span class="line">hadoop@master:~$ source /home/hadoop/.bashrc</span><br></pre></td></tr></table></figure>

<h3 id="new-and-save-the-java-file-8"><a href="#new-and-save-the-java-file-8" class="headerlink" title="new and save the .java file^[8]^"></a>new and save the .java file^[8]^</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop<span class="meta">@master</span>:~$ vi WordCount.java</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCount</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TokenizerMapper</span></span><br><span class="line">       <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">      <span class="type">StringTokenizer</span> <span class="variable">itr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString());</span><br><span class="line">      <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">        word.set(itr.nextToken());</span><br><span class="line">        context.write(word, one);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">IntSumReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text,IntWritable,Text,IntWritable&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">      <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">        sum += val.get();</span><br><span class="line">      &#125;</span><br><span class="line">      result.set(sum);</span><br><span class="line">      context.write(key, result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;word count&quot;</span>);</span><br><span class="line">    job.setJarByClass(WordCount.class);</span><br><span class="line">    job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">    job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">    job.setReducerClass(IntSumReducer.class);</span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">    System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="create-a-jar-and-run-the-jar"><a href="#create-a-jar-and-run-the-jar" class="headerlink" title="create a jar and run the jar"></a>create a jar and run the jar</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ hadoop com.sun.tools.javac.Main WordCount.java</span><br><span class="line">hadoop@master:~$ jar cf wc.jar WordCount*.class</span><br><span class="line">hadoop@master:~$ hadoop jar wc.jar WordCount /user/hadoop/shakespeare /user/hadoop/shakespeare-output-java</span><br></pre></td></tr></table></figure>

<p>![截屏2023-01-21 上午10.58.25](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午10.58.25.png)</p>
<p>Total running time is 14386(ms), faster than the python program</p>
]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/2022/12/12/cnn/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>线性回归推导及实现</title>
    <url>/2022/07/19/cs229n/</url>
    <content><![CDATA[<p>有监督学习，目的是构建一种算法，来完成$ h:x\rightarrow y $的映射，$ x^i $为输入特征，$ y^i $ 为预测类别，${(x^i,y^i);i&#x3D;1…n}$构成训练集。当$ y $ 为连续值时，该问题是一种回归问题，映射$h(x)$为与真实值$y$接近的预测值。最简单的有监督学习的回归问题是线性回归。</p>
<span id="more"></span>

<h1 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h1><p>  以预测房屋价格来描述一个线性回归问题。以房屋价格数据集为例，数据集包含房屋的大小与价格。</p>
<table>
<thead>
<tr>
<th>大小size</th>
<th>价格price</th>
</tr>
</thead>
<tbody><tr>
<td>2104</td>
<td>400</td>
</tr>
<tr>
<td>1600</td>
<td>330</td>
</tr>
<tr>
<td>2400</td>
<td>369</td>
</tr>
<tr>
<td>1416</td>
<td>232</td>
</tr>
</tbody></table>
<p>在这一问题中，房屋的价格可以通过房屋的大小进行预测，</p>
<p> ![Untitled Diagram.drawio](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Downloads&#x2F;Untitled Diagram.drawio.png)<br>其中假设$h(x)$表示为$h(x)&#x3D;\theta_0+\theta_1x $,，$x$为房屋的大小，$h(x)$为预测得到的房屋价格。当把问题进行补充，将输入特征增加为居住面积与卧室数目，这样输入特征$x$就变为一个二维向量。为简化多维向量的回归方程的表达，$h(x)$可以表示为<br>$$<br>h(x)&#x3D;\sum_{j &#x3D; 0}{n}\theta_jx_j&#x3D;\theta^Tx<br>$$</p>
<h1 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h1><p>当预测值$h(x)$越趋进于真实值$y$时，回归方程的预测效果越好，为了度量预测值与真实值之间的距离，采用最小二乘函数$J(\theta)$又称作MSE或L2损失函数。<br>$$<br>J(\theta) &#x3D; \frac{1}{2}\sum_{i &#x3D; 0} ^m(y^i - h_\theta (x^i))^2<br>$$</p>
<p>是回归方程达到最优值的方法即为使损失函数最小化。即<br>$$<br>min_{\theta}J(\theta) &#x3D; \frac{1}{2}\sum_{i &#x3D; 0} ^m(y^i - h_\theta (x^i))^2<br>$$<br>下面给出参数$\theta$的数值解法与梯度下降解法</p>
<h1 id="参数求解"><a href="#参数求解" class="headerlink" title="参数求解"></a>参数求解</h1><p>如果最小二乘函数$J(\theta)$存在全局最小值，那么存在$\hat{\theta}$使得它对于$\theta$的偏导数为0即<br>$$<br>\nabla_{\theta}J(\hat\theta))&#x3D;0<br>$$<br>求解过程如下<br>$$<br>J(\theta) &#x3D; \frac{1}{2}\sum_{i &#x3D; 0} ^m(y^i - h_\theta (x^i))^2&#x3D;\frac{1}{2}\sum_{i &#x3D; 0} ^m(\theta^Tx^i-y^i)^2<br>$$<br>根据矩阵的计算改写$J(\theta)$<br>$$<br>\begin{aligned}<br>J(\theta)<br>&amp; &#x3D; \frac{1}{2}\sum_{i &#x3D; 0} ^m(\theta^Tx^i-y^i)^T(\theta^Tx^i-y^i) \<br>&amp; &#x3D; \frac{1}{2}(\theta^Tx^Tx\theta-y\theta^Tx^T-y^Tx\theta+y^Ty)\<br>&amp; &#x3D; \frac{1}{2}(\theta^Tx^Tx\theta-2\theta^Tx^Ty+y^Ty)<br>\end{aligned}<br>$$</p>
<p>$$<br>\hat{\theta}&#x3D;argminJ(\theta)<br>$$</p>
<p>对$\theta$求偏导，导数为0时为极小值点，令$\frac{\partial J(\theta)}{\partial\theta}&#x3D;0$求得参数$\theta$的值。<br>$$<br>\frac{\partial J(\theta)}{\partial\theta}&#x3D;\frac{1}{2}(2x^Tx\theta-2x^Ty)&#x3D;0<br>$$<br>参数$\theta$的表达式为<br>$$<br>\theta&#x3D;(x^Tx)^{-1}x^Ty<br>$$</p>
<h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><p>对于使得最小二乘损失函数最优化的方法，采用<strong>梯度下降</strong>方法，梯度下降法通过不断在目标函数的梯度方向上修正参数来达到最优解。梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度），即<br>$$<br>\theta_j:&#x3D;\theta_j-\alpha\frac{\partial J(\theta)}{\partial\theta_j}<br>$$<br>$\alpha$为学习率。迭代过程为：1、选取初始值$\theta$   2、在梯度方向上修正$\theta$   3、直到$\theta_j$收敛至局部最优</p>
<h2 id="梯度下降的推导"><a href="#梯度下降的推导" class="headerlink" title="梯度下降的推导"></a>梯度下降的推导</h2><p>$$<br>\begin{aligned}<br>\frac{\partial J(\theta)}{\partial\theta_j}<br>&amp; &#x3D; (h_\theta(x)-y) \frac{\partial}{\partial\theta_j}（h_\theta(x)-y) \<br>&amp; &#x3D; (h_\theta(x)-y) \frac{\partial}{\partial\theta_j}(\sum_{j&#x3D;0}^n\theta_jx_j-y) \<br>&amp; &#x3D; (h_\theta(x)-y)x_j<br>\end{aligned}<br>$$</p>
<h1 id="线性回归的sklearn实现"><a href="#线性回归的sklearn实现" class="headerlink" title="线性回归的sklearn实现"></a>线性回归的sklearn实现</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data1=pd.read_csv(<span class="string">&#x27;ex1data1.txt&#x27;</span>,header=<span class="literal">None</span>)</span><br><span class="line">data1.head()</span><br><span class="line"></span><br><span class="line">x = data1.iloc[:,<span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line">y = data1.iloc[:,<span class="number">1</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">data1.info()</span><br><span class="line">plt.scatter(x,y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">clf = LinearRegression()</span><br><span class="line">pred = clf.fit(x,y)</span><br><span class="line"><span class="built_in">print</span>(clf.coef_)<span class="comment">#系数</span></span><br><span class="line"><span class="built_in">print</span>(clf.intercept_)<span class="comment">#截距</span></span><br><span class="line">yy = clf.predict(x)</span><br><span class="line">plt.figure(figsize = (<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line">plt.scatter(x,y,label = <span class="string">&#x27;train_data&#x27;</span>)</span><br><span class="line">plt.plot(x,yy,<span class="string">&#x27;r&#x27;</span>,label = <span class="string">&#x27;linear_regression&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()`</span><br></pre></td></tr></table></figure>

<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/07/15/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>HQL性能调优</title>
    <url>/2022/07/17/hql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</url>
    <content><![CDATA[<h1 id="hive简介"><a href="#hive简介" class="headerlink" title="hive简介"></a>hive简介</h1><p>Hive是基于Hadoop的数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单sql查询功能，可以将sql语句转换为MapReduce任务进行运行。<br>sql命令-&gt;HIVE处理，转换为MapReduce-&gt;提交任务到Hadoop,HDFS,MapReduce运行</p>
<h1 id="hive-ddl"><a href="#hive-ddl" class="headerlink" title="hive-ddl"></a>hive-ddl</h1><h3 id="hive建表（压缩表和非压缩表）"><a href="#hive建表（压缩表和非压缩表）" class="headerlink" title="hive建表（压缩表和非压缩表）"></a>hive建表（压缩表和非压缩表）</h3><p>一个表有一个或多个分区，每个分区以文件夹的形式单独存放在表文件夹的目录下</p>
<p>创建表，指定EXTERNAL是外部表，没有指定内部表，内部表在drop时会从HDFS上删除数据，外部表不会删除。</p>
<p>如果不指定数据库，hive把表创建在default数据库下</p>
<p>内部表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> login(</span><br><span class="line">userid <span class="type">bigint</span>,</span><br><span class="line">ip string</span><br><span class="line"><span class="type">time</span> <span class="type">bigint</span>)</span><br><span class="line"><span class="keyword">partition</span> <span class="keyword">by</span>(dt string)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure>
<p>外部表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">location</span><br></pre></td></tr></table></figure>
<h1 id="hive-dml"><a href="#hive-dml" class="headerlink" title="hive-dml"></a>hive-dml</h1><h3 id="hive优化的根本思想"><a href="#hive优化的根本思想" class="headerlink" title="hive优化的根本思想"></a>hive优化的根本思想</h3><p>尽早过滤数据，减少每个阶段的数据量<br>减少job数<br>解决数据倾斜问题</p>
<h3 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h3><p><strong>1、列裁剪</strong><br>只选择需要的字段<br><strong>2、分区裁剪</strong><br>查询过程中减少不必要的分区</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(orderid)</span><br><span class="line"><span class="keyword">from</span> order_table</span><br><span class="line"><span class="keyword">where</span> to_date(sale_time)<span class="operator">=</span><span class="string">&#x27;2014-03-03&#x27;</span> <span class="keyword">and</span></span><br><span class="line"><span class="keyword">hour</span>(to_date(sale_time))<span class="operator">=</span><span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>改为</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(orderid)</span><br><span class="line"><span class="keyword">from</span> order_table</span><br><span class="line"><span class="keyword">where</span> dt<span class="operator">=</span><span class="string">&#x27;2014-03-03&#x27;</span> <span class="keyword">and</span> to_date(sale_time)<span class="operator">=</span><span class="string">&#x27;2014-03-03&#x27;</span> <span class="keyword">and</span></span><br><span class="line"><span class="keyword">hour</span>(to_date(sale_time))<span class="operator">=</span><span class="number">10</span></span><br></pre></td></tr></table></figure>
<p><code>explain dependency</code>语法获取扫描的分区<br>**3、利用hive优化机制减少job数 **<br>三个以上表关联时，如果join的key相同，都会合并成一个mapreduce任务</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select a.val,b.val,c.val</span><br><span class="line">from a join b on(a.key=b.key1)</span><br><span class="line">join c on (c.key=b.key1)</span><br><span class="line">#优化为一个job</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>HIVE</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>/2023/04/04/mapreduce/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>在谷歌云平台搭建kubernetes</title>
    <url>/2023/03/15/k8s/</url>
    <content><![CDATA[<p>kubernetes是一个可扩展的管理容器化服务的平台，本文将演示如何在谷歌云平台搭建k8s</p>
<span id="more"></span>

<h1 id="搭建单节点k8s集群"><a href="#搭建单节点k8s集群" class="headerlink" title="搭建单节点k8s集群"></a>搭建单节点k8s集群</h1><p>在 GCP上新建一个虚拟机</p>
<p><img src="/2023/03/15/k8s/img1.png"></p>
<p>设置防火墙规则，仅允许cuhk段ip访问。</p>
<p>本地设置密钥对，远程连接到服务器。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh -i id_rsa shiyuzhuo@34.92.43.56  </span><br></pre></td></tr></table></figure>

<p>下载安装docker</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# sudo apt-get update</span><br><span class="line">root@instancek8s:~# sudo apt-get install ca-certificates curl gnupg lsb-release</span><br><span class="line">root@instancek8s:~# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg</span><br><span class="line">root@instancek8s:~# echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu (lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br><span class="line">root@instancek8s:~# sudo apt-get update</span><br><span class="line">root@instancek8s:~# sudo apt-get install docker-ce docker-ce-cli containerd.io -y</span><br></pre></td></tr></table></figure>

<p>下载安装 kubedam kubectl 和 kubelet</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# sudo apt-get update</span><br><span class="line">root@instancek8s:~# sudo apt-get install -y apt-transport-https ca-certificates curl</span><br><span class="line">root@instancek8s:~# sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg</span><br><span class="line">root@instancek8s:~# echo &quot;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">root@instancek8s:~# sudo apt-get update</span><br><span class="line">root@instancek8s:~# sudo apt-get install -y kubelet kubeadm kubectl</span><br><span class="line">root@instancek8s:~# sudo apt-mark hold kubelet kubeadm kubectl</span><br></pre></td></tr></table></figure>

<p>配置k8s</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# sudo ufw disable</span><br><span class="line">root@instancek8s:~# sudo swapoff -a; sed -i &#x27;/swap/d&#x27; /etc/fstab</span><br><span class="line">root@instancek8s:~# sudo systemctl daemon-reload</span><br><span class="line">root@instancek8s:~# sudo systemctl restart docker</span><br><span class="line">root@instancek8s:~# sudo systemctl restart kubelet</span><br><span class="line">root@instancek8s:~# cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf</span><br><span class="line">br_netfilter</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">root@instancek8s:~# cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">root@instancek8s:~# sudo sysctl --system</span><br><span class="line">root@instancek8s:~# echo 1 &gt; /proc/sys/net/ipv4/ip_forward</span><br></pre></td></tr></table></figure>

<p>初始化</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# rm /etc/containerd/config.toml</span><br><span class="line">root@instancek8s:~# systemctl restart containerd</span><br><span class="line">root@instancek8s:~# kubeadm init</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/15/k8s/img2.png"></p>
<p>设置节点为master，在节点上部署pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line">root@instancek8s:~# wget https://docs.projectcalico.org/v3.25/manifests/calico.yaml --no-check-certificate</span><br><span class="line">root@instancek8s:~# kubectl apply -f calico.yaml</span><br></pre></td></tr></table></figure>

<p>删除 taint</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# kubectl taint nodes --all node-role.kubernetes.io/control-plane-</span><br></pre></td></tr></table></figure>

<p>部署hello world yaml</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# kubectl apply -f https://mobitec.ie.cuhk.edu.hk/iems5730Spring2023/static_files/assignments/hello-world-demo.yaml</span><br></pre></td></tr></table></figure>

<p>检查pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# kubectl get pods</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/15/k8s/img3.png"></p>
<p>访问30123端口</p>
<p><img src="/2023/03/15/k8s/img4.png"></p>
<h1 id="Kubernetes-集群搭建"><a href="#Kubernetes-集群搭建" class="headerlink" title="Kubernetes 集群搭建"></a>Kubernetes 集群搭建</h1><p>在 GCP上按照相同的配置设置四台虚拟机</p>
<p><img src="/2023/03/15/k8s/img5.png"></p>
<p>与上一步相同，下载与安装docker，kubedam kubectl 和 kubelet</p>
<p>初始化k8s， 在mater节点部署pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# kubeadm init</span><br><span class="line">root@instancek8s:~# export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line">root@instancek8s:~# wget https://docs.projectcalico.org/v3.25/manifests/calico.yaml --no-check-certificate</span><br><span class="line">root@instancek8s:~# kubectl apply -f calico.yaml</span><br></pre></td></tr></table></figure>

<p>在集群中加入其他节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@k8smaster:~# kubeadm token create --print-join-command</span><br><span class="line">kubeadm join 10.170.0.5:6443 --token 98hfcy.wcggumrd54d79zku --discovery-token-ca-cert-hash sha256:3450e1a3523d39a20f5b6e2e2ee94a003cb3537d2a47fbf7f28b6fff07eaa660 </span><br></pre></td></tr></table></figure>

<p>在其他三个节点运行join command</p>
<p><img src="/2023/03/15/k8s/img6.png"></p>
<p><img src="/2023/03/15/k8s/img7.png"></p>
<p><img src="/2023/03/15/k8s/img8.png"></p>
<p><img src="/2023/03/15/k8s/img9.png"></p>
<p>在master节点上部署hadoop</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@k8smaster:~# kubectl create -f https://mobitec.ie.cuhk.edu.hk/iems5730Spring2023/static_files/assignments/hadoop.yaml</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/15/k8s/img10.png"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@k8smaster:~# kubectl get pods</span><br><span class="line">root@k8smaster:~# kubectl get service</span><br><span class="line">root@k8smaster:~# kubectl get pods -o wide</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/15/k8s/img11.png"></p>
<h1 id="试用谷歌云Serverless搭建Kubernetes-服务"><a href="#试用谷歌云Serverless搭建Kubernetes-服务" class="headerlink" title="试用谷歌云Serverless搭建Kubernetes 服务"></a>试用谷歌云Serverless搭建Kubernetes 服务</h1><p>在 GKE上建立一个k8s集群</p>
<p><img src="/2023/03/15/k8s/img12.png"></p>
<p>连接到集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gcloud container clusters get-credentials cluster-1 --zone asia-east1-a --project multi-node-375214</span><br></pre></td></tr></table></figure>

<p>部署hadoop.yaml </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl create -f https://mobitec.ie.cuhk.edu.hk/iems5730Spring2023/static_files/assignments/hadoop.yaml</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/15/k8s/img13.png"></p>
<p>访问端口32007</p>
<p><img src="/2023/03/15/k8s/img14.png"></p>
<p>运行Hadoop Terasort</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl exec --stdin --tty hdfs-master -- /bin/bash</span><br><span class="line"> cd /usr/local/hadoop</span><br><span class="line"></span><br><span class="line"> ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teragen 20000000 terasort/input2GB</span><br><span class="line"> ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar terasort terasort/input2GB terasort/output2GB</span><br><span class="line"> ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teravalidate terasort/output2GB terasort/check2GB</span><br><span class="line"></span><br><span class="line"> ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teragen 200000000 terasort/input20GB </span><br><span class="line"> ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar terasort terasort/input20GB terasort/output20GB</span><br><span class="line"> ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teravalidate terasort/output20GB terasort/check20GB </span><br></pre></td></tr></table></figure>

<p> ![截屏2023-02-28 下午8.57.18](&#x2F;Users&#x2F;shiyuzhuo&#x2F;dwxa&#x2F;source&#x2F;_posts&#x2F;k8s&#x2F;截屏2023-02-28 下午8.57.18.png)</p>
<p>![截屏2023-02-28 下午8.57.42](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Documents&#x2F;截屏2023-02-28 下午8.57.42.png)</p>
<p>运行时间</p>
<p>2GB:40+269+46 &#x3D; 355s</p>
<p>20GB:739+14839+288 &#x3D; 15866s</p>
<h1 id="Fault-tolerance"><a href="#Fault-tolerance" class="headerlink" title="Fault-tolerance"></a>Fault-tolerance</h1><p>Kill hello-world pods</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# kubectl delete pods hello-world-b6mj2</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/15/k8s/img15.png"></p>
<p>kill pods hello-world-b6mj2 之后，一个新的pod hello-world-p4pr6 出现.在第一个pod失败后，将有一个具有相同配置的新pod来完成容错</p>
<p>Reference:</p>
<p>[1]<a href="https://docs.docker.com/engine/install/ubuntu/">https://docs.docker.com/engine/install/ubuntu/</a></p>
<p>[2]<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a></p>
<p>[3]<a href="https://blog.csdn.net/qq_43580215/article/details/125153959">https://blog.csdn.net/qq_43580215/article/details/125153959</a></p>
<p>[4]<a href="https://stackoverflow.com/questions/51121136/the-connection-to-the-server-localhost8080-was-refused-did-you-specify-the-ri">https://stackoverflow.com/questions/51121136/the-connection-to-the-server-localhost8080-was-refused-did-you-specify-the-ri</a></p>
<p>[5]<a href="https://cloud.tencent.com/document/product/457/42948">https://cloud.tencent.com/document/product/457/42948</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>/2022/10/29/pca/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>raft</title>
    <url>/2023/02/07/raft/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/2022/12/12/raft%E8%AE%BA%E6%96%87/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/2022/12/12/rnn/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/2022/12/12/transformer/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>假设检验</title>
    <url>/2022/07/20/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</url>
    <content><![CDATA[<h2 id="参数估计与假设检验"><a href="#参数估计与假设检验" class="headerlink" title="参数估计与假设检验"></a>参数估计与假设检验</h2><p>参数估计讨论的是用样本估计总体参数的方法，总体参数μ在估计前是未知的。<br>而在假设检验中，则是先对μ的值提出一个假设，然后利用样本信息去检验这个假设是否成立。</p>
<span id="more"></span>

<h2 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h2><p>假设检验：假设就是对从总体参数(均值、比例等)的具体数值所作的陈述，比如，我认为配方一比配方二的效果要好。<strong>而假设检验就是先对总体的参数提出某种假设，然后利用样本的信息判断假设是否成立的过程</strong>，比如上面的假设信息我该接受还是拒绝。</p>
<h2 id="显著性水平"><a href="#显著性水平" class="headerlink" title="显著性水平"></a>显著性水平</h2><p><strong>显著性水平是一个概率值，原假设为真时，拒绝原假设的概率，表示为α，常取值为0.05、0.01、0.10。</strong>一个公司招聘，本来准备招聘100个人，公司希望只有5%的人是混水摸鱼招聘进来，所以可能会有5个人混进来，所谓显著性水平α，就是你允许有多少比例混水摸鱼的能通过测试。</p>
<h2 id="检验统计量"><a href="#检验统计量" class="headerlink" title="检验统计量"></a>检验统计量</h2><p><strong>即计算检验的统计量。根据给定的显著性水平，查表得出相应的临界值。再将检验统计量的值与该显著性水平的临界值进行比较</strong>，得出是否拒绝原假设的结论。</p>
<h2 id="假设检验的两种错误"><a href="#假设检验的两种错误" class="headerlink" title="假设检验的两种错误"></a>假设检验的两种错误</h2><h3 id="弃真"><a href="#弃真" class="headerlink" title="弃真"></a>弃真</h3><p><em><strong>类型 I 错误(弃真)，如原假设为真，但否定它，则会犯类型 I 错误。犯类型 I 错误的概率为 α（即您为假设检验设置的显著性水平）。</strong></em>α 为 0.05 表明，当您否定原假设时，您愿意接受 5% 的犯错概率。</p>
<h3 id="取伪"><a href="#取伪" class="headerlink" title="取伪"></a>取伪</h3><p><em><strong>原假设实际上是不正确的，但是我们却做出了接受原假设的决定，此类错误称为第二类错误</strong></em></p>
<h2 id="单双侧检验"><a href="#单双侧检验" class="headerlink" title="单双侧检验"></a>单双侧检验</h2><p>当假设关键词有不得少于&#x2F;低于的时候用左侧检验，比如灯泡的使用寿命不得少于&#x2F;低于700小时时；当假设关键词有不得多于&#x2F;高于的时候用右侧检验，比如次品率不得多于&#x2F;高于5%时。双侧检验指按分布两端计算显著性水平概率的检验，应用于理论上不能确定两个总体一个一定比另一个大或小的假设检验。</p>
<h2 id="假设检验结果"><a href="#假设检验结果" class="headerlink" title="假设检验结果"></a>假设检验结果</h2><p>一般假设检验写作<em>H0：μ1&#x3D;μ2</em>。<br>单侧，若<em>p值&gt;α</em>,不拒绝H0，若p值&lt;α,拒绝$H_0$；双侧，若<em>p值&gt;1&#x2F;2α</em>,不拒绝$H_0$，若<em>p值&lt;1&#x2F;2α</em>,拒绝$H_0$<br>假设检验方法：z检验，t检验，卡方检验<br>两独立样本t检验(ab实验背后原理)</p>
<h2 id="Q-Q图"><a href="#Q-Q图" class="headerlink" title="Q-Q图"></a>Q-Q图</h2><p>Q-Q图鉴别样本数据是否近似正态分布，样本数据近似落在一条直线附近，则近似于正态分布，直线斜率为<em><strong>标准差</strong></em>，截距为<em><strong>均值</strong></em></p>
]]></content>
      <categories>
        <category>统计学原理</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>/2022/12/12/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>数据探索性分析（EDA）</title>
    <url>/2022/08/30/%E6%95%B0%E6%8D%AEEDA/</url>
    <content><![CDATA[<p>数据探索性分析往往是我们在进行数据建模的第一步，数据的质量也决定着模型效果的上限，因此数据探索性分析也是解决数据科学问题中的重要一环，下面介绍数据探索性分析的简要流程及几种常见处理方法。以阿里云天池学习赛工业蒸汽数据集为例。</p>
<span id="more"></span>

<h1 id="数据信息探索（缺失值，异常值，数据分布）"><a href="#数据信息探索（缺失值，异常值，数据分布）" class="headerlink" title="数据信息探索（缺失值，异常值，数据分布）"></a>数据信息探索（缺失值，异常值，数据分布）</h1><h2 id="数据分布"><a href="#数据分布" class="headerlink" title="数据分布"></a>数据分布</h2><h2 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h2><h2 id="异常值处理"><a href="#异常值处理" class="headerlink" title="异常值处理"></a>异常值处理</h2><h1 id="特征相关性"><a href="#特征相关性" class="headerlink" title="特征相关性"></a>特征相关性</h1><h1 id="多重共线性"><a href="#多重共线性" class="headerlink" title="多重共线性"></a>多重共线性</h1><h1 id="数据降维"><a href="#数据降维" class="headerlink" title="数据降维"></a>数据降维</h1><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><h1 id="归一化处理"><a href="#归一化处理" class="headerlink" title="归一化处理"></a>归一化处理</h1><h2 id="最大最小归一化"><a href="#最大最小归一化" class="headerlink" title="最大最小归一化"></a>最大最小归一化</h2><h2 id="Z-Score归一化"><a href="#Z-Score归一化" class="headerlink" title="Z-Score归一化"></a>Z-Score归一化</h2>]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
  </entry>
  <entry>
    <title>常见机器学习算法sklearn实现demo</title>
    <url>/2022/10/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0demo/</url>
    <content><![CDATA[<p>导入相关的库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">from</span> kneed <span class="keyword">import</span> KneeLocator</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br></pre></td></tr></table></figure>

<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>LinearRegression()线性回归，十折交叉验证，计算评价指标MSE</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">problem_5</span>(<span class="params">filename,predictors,target</span>):</span><br><span class="line">    <span class="comment"># write your logic here, model is the MLR model</span></span><br><span class="line">    df = pd.read_csv(filename)</span><br><span class="line">    X = df[predictors].values.astype(<span class="built_in">float</span>)</span><br><span class="line">    y = df[target].values.astype(<span class="built_in">float</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    sc_X = StandardScaler()</span><br><span class="line">    sc_y = StandardScaler()</span><br><span class="line">    X = sc_X.fit_transform(X)</span><br><span class="line">    y = sc_y.fit_transform(y)</span><br><span class="line">    mse = []</span><br><span class="line">    regr = linear_model.LinearRegression()</span><br><span class="line">    kf = KFold(n_splits=<span class="number">10</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">5726</span>)</span><br><span class="line">    <span class="keyword">for</span> X_train, X_test <span class="keyword">in</span> kf.split(X):</span><br><span class="line">        regr.fit(X[X_train], y[X_train])</span><br><span class="line">        y_pred = regr.predict(X[X_test])</span><br><span class="line">        mse.append(mean_squared_error(y[X_test], y_pred))</span><br><span class="line">    mean_cv_mse = np.mean(mse)</span><br><span class="line">    sd_cv_mse = np.std(mse)</span><br><span class="line">    model = regr</span><br><span class="line">    <span class="keyword">return</span> model, mean_cv_mse, sd_cv_mse</span><br></pre></td></tr></table></figure>

<h1 id="随机森林分类"><a href="#随机森林分类" class="headerlink" title="随机森林分类"></a>随机森林分类</h1><p>随机森林分类器，十折交叉验证，评价指标准确率</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">problem_3</span>(<span class="params">filename,predictors,target</span>):</span><br><span class="line">    <span class="comment"># write your logic here, model is the RF model</span></span><br><span class="line">    df = pd.read_csv(filename)</span><br><span class="line">    train_data = df[predictors]</span><br><span class="line">    label = df[target]</span><br><span class="line">    PredictorScaler = StandardScaler()</span><br><span class="line">    PredictorScalerFit = PredictorScaler.fit(train_data)</span><br><span class="line">    x_train = PredictorScalerFit.transform(train_data)</span><br><span class="line">    model = RandomForestClassifier(random_state=<span class="number">5726</span>)</span><br><span class="line">    acc_arr = cross_val_score(model, x_train, label, scoring=<span class="string">&#x27;accuracy&#x27;</span>, cv=<span class="number">10</span>)</span><br><span class="line">    mean_cv_acc = np.mean(acc_arr)</span><br><span class="line">    sd_cv_acc = np.std(acc_arr)</span><br><span class="line">    <span class="keyword">return</span> model, mean_cv_acc, sd_cv_acc</span><br></pre></td></tr></table></figure>

<h1 id="支持向量回归"><a href="#支持向量回归" class="headerlink" title="支持向量回归"></a>支持向量回归</h1><p>评价指标MAE,RMSE</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">problem_4</span>(<span class="params">filename,predictors,target</span>):</span><br><span class="line">    <span class="comment"># write your logic here, model is the SVR model</span></span><br><span class="line">    df = pd.read_csv(filename)</span><br><span class="line">    X = df[predictors].values.astype(<span class="built_in">float</span>)</span><br><span class="line">    y = df[target].values.astype(<span class="built_in">float</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    sc_X = StandardScaler()</span><br><span class="line">    sc_y = StandardScaler()</span><br><span class="line">    X = sc_X.fit_transform(X)</span><br><span class="line">    y = sc_y.fit_transform(y)</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>,</span><br><span class="line">                                                        random_state=<span class="number">5726</span>)</span><br><span class="line">    regressor = SVR(kernel=<span class="string">&#x27;poly&#x27;</span>)</span><br><span class="line">    regressor.fit(X_train, y_train)</span><br><span class="line">    y_pred = regressor.predict(X_test)</span><br><span class="line">    test_mae = mean_absolute_error(y_test, y_pred)</span><br><span class="line">    test_rmse = mean_squared_error(y_test, y_pred) ** <span class="number">0.5</span></span><br><span class="line">    model = regressor</span><br><span class="line">    <span class="keyword">return</span> model, test_mae, test_rmse</span><br></pre></td></tr></table></figure>

<h1 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">problem_6</span>(<span class="params">train_filename,predictors,test_filename</span>):</span><br><span class="line">    <span class="comment"># write your logic here, model is the k-mean model</span></span><br><span class="line">    df_train = pd.read_csv(train_filename)</span><br><span class="line">    df_test = pd.read_csv(test_filename)</span><br><span class="line">    train_data = df_train[predictors]</span><br><span class="line">    test_data = df_test[predictors]</span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    scaled_features = scaler.fit_transform(train_data)</span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    scaled_features2 = scaler.fit_transform(test_data)</span><br><span class="line">    kmeans_kwargs = &#123;<span class="string">&quot;init&quot;</span>: <span class="string">&quot;random&quot;</span>, <span class="string">&quot;n_init&quot;</span>: <span class="number">5</span>, <span class="string">&quot;max_iter&quot;</span>: <span class="number">300</span>, <span class="string">&quot;random_state&quot;</span>: <span class="number">5726</span>&#125;</span><br><span class="line">    sse = []  <span class="comment"># A list holds the SSE values for each k</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)</span><br><span class="line">        kmeans.fit(scaled_features)</span><br><span class="line">        sse.append(kmeans.inertia_)</span><br><span class="line">    kl = KneeLocator(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>), sse, curve=<span class="string">&quot;convex&quot;</span>, direction=<span class="string">&quot;decreasing&quot;</span>)</span><br><span class="line">    kmeans = KMeans(init=<span class="string">&quot;random&quot;</span>, n_clusters=<span class="number">4</span>, n_init=<span class="number">5</span>, random_state=<span class="number">5726</span>)</span><br><span class="line">    kmeans.fit(scaled_features)</span><br><span class="line">    result = kmeans.predict(scaled_features2)</span><br><span class="line">    k = kl.elbow</span><br><span class="line">    model = kmeans</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model, k, result</span><br></pre></td></tr></table></figure>

<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">problem_2</span>(<span class="params">filename,predictors,target</span>):</span><br><span class="line">    <span class="comment"># write your logic here, model is the NN model</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup_seed</span>(<span class="params">seed</span>):</span><br><span class="line">        torch.manual_seed(seed)</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br><span class="line">        np.random.seed(seed)</span><br><span class="line">        random.seed(seed)</span><br><span class="line">        torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    setup_seed(<span class="number">5726</span>)</span><br><span class="line">    df = pd.read_csv(filename)</span><br><span class="line">    train_data = df[predictors]</span><br><span class="line">    label = df[target]</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(train_data, label, test_size=<span class="number">0.3</span>,</span><br><span class="line">                                                        random_state=<span class="number">5726</span>)</span><br><span class="line">    PredictorScaler = StandardScaler()</span><br><span class="line">    PredictorScalerFit = PredictorScaler.fit(X_train)</span><br><span class="line">    x_train = PredictorScalerFit.transform(X_train)</span><br><span class="line">    PredictorScaler = StandardScaler()</span><br><span class="line">    PredictorScalerFit = PredictorScaler.fit(X_test)</span><br><span class="line">    x_test = PredictorScalerFit.transform(X_test)</span><br><span class="line">    n_input, n_hidden1, n_hidden2, n_out, learning_rate = <span class="number">11</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">0.01</span></span><br><span class="line">    data_x = torch.FloatTensor(x_train)</span><br><span class="line">    y_train = np.array(y_train)</span><br><span class="line">    data_y = torch.FloatTensor(y_train).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">    model = nn.Sequential(nn.Linear(n_input, n_hidden1), nn.ReLU(),</span><br><span class="line">                          nn.Linear(n_hidden1, n_hidden2), nn.ReLU(),</span><br><span class="line">                          nn.Linear(n_hidden2, n_out), nn.Sigmoid())</span><br><span class="line">    loss_function = nn.MSELoss()</span><br><span class="line">    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line">    losses = []  <span class="comment"># Training Loop</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">        pred_y = model(data_x)</span><br><span class="line">        loss = loss_function(pred_y, data_y)</span><br><span class="line">        losses.append(loss.item())</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    targets = model(torch.FloatTensor(x_test))</span><br><span class="line">    test_recall = metrics.precision_score(np.<span class="built_in">round</span>(targets.detach().numpy()), y_test)</span><br><span class="line">    test_precision = metrics.recall_score(np.<span class="built_in">round</span>(targets.detach().numpy()), y_test)</span><br><span class="line">    model = model._modules</span><br><span class="line">    <span class="keyword">return</span> model, test_precision, test_recall</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>/2022/10/29/%E6%A0%91%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>逻辑回归</title>
    <url>/2022/07/27/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h1 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h1><p>对于二分类问题，我们期望通过监督学习，输出结果达到二分类的效果，如0代表负向分类，1代表正向分类。因此，所选择的函数的输出范围需要在[0,1]内，而对于线性回归模型，它的输出是离散的，因此我们要把离散的输出限制在连续范围[0,1]内。</p>
<p>有如下函数<br>$$<br>h(x)&#x3D;g(\theta^{T}x)&#x3D;\frac{1}{1+e^{-\theta^{T}x}}<br>$$</p>
<p>即<br>$$<br>g(z)&#x3D;\frac{1}{1+e^{-z}}<br>$$<br>其中$\theta^Tx$是线性函数，范围为$[-\infty,\infty]$，$g(z)$将$[-\infty,\infty]$的范围映射到$[0,1]$，通过这种方法，我门可以采用类似回归的方法解决二分类问题，$g(z)$的值被视为分到某一类的概率，即分到1或0类的概率为<br>$$<br>P(y&#x3D;1|x;\theta)&#x3D;h(x)&#x3D;g(\theta^{T}x)\<br>P(y&#x3D;0|x;\theta)&#x3D;1-h(x)&#x3D;1-g(\theta^{T}x)<br>$$<br>将上式可以更简洁地表示为<br>$$<br>P(y|x;\theta)&#x3D;（h(x))^y(1-h(x))^{1-y}<br>$$</p>
<h1 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h1><p>样本$(x_i,y_i)$满足独立同分布，因此，采用极大似然估计法估计参数$\theta$，首先写出似然函数<br>$$<br>\begin{aligned}<br>{ \max_{\theta}L(\theta)}<br>&amp; &#x3D; \prod_{i&#x3D;1}^mp(y^{(i)}|x^{(i)};\theta)\<br>&amp; &#x3D; \prod_{i&#x3D;1}^m(h(x^{(i)}))^{y^{(i)}}(1-h(x^{(i)}))^{1-y^{(i)}}<br>\end{aligned}<br>$$<br>对似然函数取对数，转换为求和<br>$$<br>l(\theta)&#x3D;logL(\theta)\<br>&#x3D;\sum_{i&#x3D;1}^my^{(i)}logh(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))<br>$$</p>
<h1 id="梯度上升法求解参数"><a href="#梯度上升法求解参数" class="headerlink" title="梯度上升法求解参数"></a>梯度上升法求解参数</h1><p>与线性回归中估计参数$\theta$的梯度下降方法类似，采用梯度上升方法，来最大化似然函数<br>$$<br>\theta:&#x3D;\theta+\alpha\nabla_{\theta}l(\theta)<br>$$</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial l(\theta)}{\partial\theta_j}<br>&amp; &#x3D; (y\frac{1}{g(\theta^{T}x)}-(1-y)\frac{1}{1-g(\theta^{T}x)})\frac{\partial g(\theta^{T}x)}{\partial\theta_j} \<br>&amp; &#x3D; (y\frac{1}{g(\theta^{T}x)}-(1-y)\frac{1}{1-g(\theta^{T}x)}) g(\theta^{T}x)(1-g(\theta^{T}x))\frac{\partial \theta^{T}x}{\partial\theta_j}\<br>&amp; &#x3D; (y(1-g(\theta^{T}x))-(1-y)g(\theta^{T}x)) x_j\<br>&amp; &#x3D; (y-h_{\theta}(x))x_j<br>\end{aligned}<br>$$</p>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>逻辑回归的损失函数即是在似然函数前加负号，即将极大化的似然函数转换为求最小值的损失函数。该损失函数又称为二进制交叉熵损失函数（BCE）<br>$$<br>H(\theta)&#x3D;-l(\theta)<br>&#x3D;-[\sum_{i&#x3D;1}^my^{(i)}logh(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))]<br>$$</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
</search>
