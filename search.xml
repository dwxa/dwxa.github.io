<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>SVM推导及Python实现</title>
    <url>/2022/07/27/CS229n%E4%B9%8BSVM/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Pandas常见使用</title>
    <url>/2022/08/10/Pandas%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E7%9A%84%E5%89%AF%E6%9C%AC/</url>
    <content><![CDATA[<h1 id="读取外部数据"><a href="#读取外部数据" class="headerlink" title="读取外部数据"></a>读取外部数据</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment">#读取csv，txt，excel文件</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;filename.csv&#x27;</span>)</span><br><span class="line">df = pd.read_table(<span class="string">&#x27;filename.csv&#x27;</span>)</span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;filename.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>常见参数 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&#x27;filename.csv&#x27;</span>，header=<span class="literal">None</span>，sep=<span class="string">&#x27;\t&#x27;</span>,index_col=<span class="string">&quot;col1&quot;</span>,usecols=[<span class="string">&quot;col1&quot;</span>,<span class="string">&quot;col2&quot;</span>],nrows=<span class="number">5</span>，dtype=&#123;<span class="string">&quot;col1&quot;</span>:<span class="built_in">str</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>默认header&#x3D;”infer”， header&#x3D;None表示第一行不作为列名，sep指定分隔符，index_col选取某列的值作为索引，usecols仅读取需要的列,nrows为读取数据的行数，dtype设置读取数据时的字段数据类型。</p>
<h1 id="数据写入文件"><a href="#数据写入文件" class="headerlink" title="数据写入文件"></a>数据写入文件</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.to_csv(<span class="string">&#x27;filename.csv&#x27;</span>,index=<span class="literal">False</span>)</span><br><span class="line">df.to_excel(<span class="string">&#x27;filename.xlsx&#x27;</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>采用to_csv将数据写入csv文件，采用to_excel将数据写入excel文件。index&#x3D;False表示将Dataframe的索引在保存文件时去除。<br>同样也可以使用to_csv将数据保存至txt文件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.to_csv(<span class="string">&#x27;filename.txt&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Dataframe操作"><a href="#Dataframe操作" class="headerlink" title="Dataframe操作"></a>Dataframe操作</h1><p>Pandas中存储二维数据的数据结构为dataframe，下述几种dataframe类型的基本操作。</p>
<h2 id="构建dataframe"><a href="#构建dataframe" class="headerlink" title="构建dataframe"></a>构建dataframe</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]],columns=[<span class="string">&#x27;col1&#x27;</span>,<span class="string">&#x27;col2&#x27;</span>])</span><br><span class="line"></span><br><span class="line">   col1  col2</span><br><span class="line"><span class="number">0</span>     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line"><span class="number">1</span>     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line"><span class="number">2</span>     <span class="number">5</span>     <span class="number">6</span></span><br></pre></td></tr></table></figure>
<h2 id="dataframe索引"><a href="#dataframe索引" class="headerlink" title="dataframe索引"></a>dataframe索引</h2><h3 id="数据列索引"><a href="#数据列索引" class="headerlink" title="数据列索引"></a>数据列索引</h3><p>通过df[‘列名’]可以取出对应的数据列，df[[‘列名1’,’列名2]]可以取出多列的数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;col1&#x27;</span>]</span><br><span class="line"><span class="number">0</span>    <span class="number">1</span></span><br><span class="line"><span class="number">1</span>    <span class="number">3</span></span><br><span class="line"><span class="number">2</span>    <span class="number">5</span></span><br><span class="line">Name: col1, dtype: int64</span><br><span class="line"></span><br><span class="line">df[[<span class="string">&#x27;col1&#x27;</span>,<span class="string">&#x27;col2&#x27;</span>]]</span><br><span class="line">col1	col2</span><br><span class="line"><span class="number">0</span>	<span class="number">1</span>	<span class="number">2</span></span><br><span class="line"><span class="number">1</span>	<span class="number">3</span>	<span class="number">4</span></span><br><span class="line"><span class="number">2</span>	<span class="number">5</span>	<span class="number">6</span></span><br></pre></td></tr></table></figure>
<h3 id="iloc索引"><a href="#iloc索引" class="headerlink" title="iloc索引"></a>iloc索引</h3><p>iloc所以可以根据行列位置对数据进行切片，返回需要的值。iloc[ : , : ] 行列切片以“，”隔开，前面的冒号是取行数，后面的冒号是取列数(冒号切片左闭右开)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]],columns=[<span class="string">&#x27;col1&#x27;</span>,<span class="string">&#x27;col2&#x27;</span>,<span class="string">&#x27;col3&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=========================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df.iloc[<span class="number">1</span>,<span class="number">1</span>])<span class="comment">#取第二行第二列</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=========================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df.iloc[[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>]])<span class="comment">#取前两行前两列</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=========================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df.iloc[<span class="number">0</span>:<span class="number">2</span>, <span class="number">1</span>:<span class="number">2</span>])<span class="comment">#取第一至二行，第二列</span></span><br><span class="line"></span><br><span class="line">   col1  col2  col3</span><br><span class="line"><span class="number">0</span>     <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span></span><br><span class="line"><span class="number">1</span>     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span></span><br><span class="line"><span class="number">2</span>     <span class="number">5</span>     <span class="number">6</span>     <span class="number">7</span></span><br><span class="line">=========================</span><br><span class="line"><span class="number">4</span></span><br><span class="line">=========================</span><br><span class="line">   col1  col2</span><br><span class="line"><span class="number">0</span>     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line"><span class="number">1</span>     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">=========================</span><br><span class="line">   col2</span><br><span class="line"><span class="number">0</span>     <span class="number">2</span></span><br><span class="line"><span class="number">1</span>     <span class="number">4</span></span><br></pre></td></tr></table></figure>
<h2 id="dataframe分组"><a href="#dataframe分组" class="headerlink" title="dataframe分组"></a>dataframe分组</h2><p>dataframe的分组操作主要依赖groupby函数完成，基本调用方式为</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby(<span class="string">&#x27;分组维度&#x27;</span>)[<span class="string">&#x27;分组数据&#x27;</span>].操作()</span><br></pre></td></tr></table></figure>
<p>主要应用于依据某列分组，统计待分组数据在该列维度下的数据值，如对于房价数据，根据<em>城市</em>分组，统计该在城市的<em>房屋价格</em>最大值。以data2.csv数据集为例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df=pd.read_csv(<span class="string">&#x27;data2.csv&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=========================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df.groupby(<span class="string">&#x27;name&#x27;</span>)[<span class="string">&#x27;C&#x27;</span>].<span class="built_in">max</span>())<span class="comment">#按name列分组对应的C列的最大值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=========================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df.groupby(<span class="string">&#x27;name&#x27;</span>)[<span class="string">&#x27;B&#x27;</span>].mean())<span class="comment">#按name列分组对应的C列的平均值</span></span><br><span class="line"></span><br><span class="line">   A     B   C    D      name</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80</span>  NaN        HK</span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75</span> -<span class="number">1.0</span>  Shanghai</span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   <span class="number">7.0</span>  <span class="number">75</span>  <span class="number">0.0</span>        HK</span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>  <span class="number">80</span> -<span class="number">1.0</span>   Beijing</span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80</span>  <span class="number">1.0</span>   Beijing</span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>  <span class="number">75</span>  NaN     Macau</span><br><span class="line">=========================</span><br><span class="line">name</span><br><span class="line">Beijing     <span class="number">80</span></span><br><span class="line">HK          <span class="number">80</span></span><br><span class="line">Macau       <span class="number">75</span></span><br><span class="line">Shanghai    <span class="number">75</span></span><br><span class="line">Name: C, dtype: int64</span><br><span class="line">=========================</span><br><span class="line">name</span><br><span class="line">Beijing     <span class="number">6.45</span></span><br><span class="line">HK          <span class="number">8.75</span></span><br><span class="line">Macau       <span class="number">8.30</span></span><br><span class="line">Shanghai    <span class="number">5.60</span></span><br><span class="line">Name: B, dtype: float64</span><br></pre></td></tr></table></figure>
<h1 id="array和dataframe相互转换"><a href="#array和dataframe相互转换" class="headerlink" title="array和dataframe相互转换"></a>array和dataframe相互转换</h1><p>array转换为dataframe，pd.DataFrame将array类型数据转换为DataFrame，默认列名为0，1…，参数columns设置列名。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = pd.DataFrame(df.values,columns=<span class="built_in">list</span>(<span class="string">&quot;AB&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line">   A  B</span><br><span class="line"><span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span></span><br><span class="line"><span class="number">1</span>  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line"><span class="number">2</span>  <span class="number">5</span>  <span class="number">6</span></span><br></pre></td></tr></table></figure>
<p>dataframe转换为array，values将dataframe格式数据转换为array对象。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.values</span><br><span class="line"></span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<h1 id="数据特征统计函数"><a href="#数据特征统计函数" class="headerlink" title="数据特征统计函数"></a>数据特征统计函数</h1><p>info()返回数据表的列，缺失值情况，数据类型等信息，以数据表HousingData.csv为例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&quot;HousingData.csv&quot;</span>)</span><br><span class="line">df.info()</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;pandas.core.frame.DataFrame&#x27;</span>&gt;</span><br><span class="line">RangeIndex: <span class="number">506</span> entries, <span class="number">0</span> to <span class="number">505</span></span><br><span class="line">Data columns (total <span class="number">14</span> columns):</span><br><span class="line"> <span class="comment">#   Column   Non-Null Count  Dtype  </span></span><br><span class="line">---  ------   --------------  -----  </span><br><span class="line"> <span class="number">0</span>   CRIM     <span class="number">486</span> non-null    float64</span><br><span class="line"> <span class="number">1</span>   ZN       <span class="number">486</span> non-null    float64</span><br><span class="line"> <span class="number">2</span>   INDUS    <span class="number">486</span> non-null    float64</span><br><span class="line"> <span class="number">3</span>   CHAS     <span class="number">486</span> non-null    float64</span><br><span class="line"> <span class="number">4</span>   NOX      <span class="number">506</span> non-null    float64</span><br><span class="line">dtypes: float64(<span class="number">12</span>), int64(<span class="number">2</span>)</span><br><span class="line">memory usage: <span class="number">55.5</span> KB</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>df.describe()返回数据表中每列的的计数，平均值，最大最小值，分位数等统计量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.describe()</span><br><span class="line"></span><br><span class="line">             CRIM          ZN       INDUS        CHAS         NOX</span><br><span class="line">count  <span class="number">486.000000</span>  <span class="number">486.000000</span>  <span class="number">486.000000</span>  <span class="number">486.000000</span>  <span class="number">506.000000</span></span><br><span class="line">mean     <span class="number">3.611874</span>   <span class="number">11.211934</span>   <span class="number">11.083992</span>    <span class="number">0.069959</span>    <span class="number">0.554695</span></span><br><span class="line">std      <span class="number">8.720192</span>   <span class="number">23.388876</span>    <span class="number">6.835896</span>    <span class="number">0.255340</span>    <span class="number">0.115878</span></span><br><span class="line"><span class="built_in">min</span>      <span class="number">0.006320</span>    <span class="number">0.000000</span>    <span class="number">0.460000</span>    <span class="number">0.000000</span>    <span class="number">0.385000</span></span><br><span class="line"><span class="number">25</span>%      <span class="number">0.081900</span>    <span class="number">0.000000</span>    <span class="number">5.190000</span>    <span class="number">0.000000</span>    <span class="number">0.449000</span></span><br><span class="line"><span class="number">50</span>%      <span class="number">0.253715</span>    <span class="number">0.000000</span>    <span class="number">9.690000</span>    <span class="number">0.000000</span>    <span class="number">0.538000</span></span><br><span class="line"><span class="number">75</span>%      <span class="number">3.560263</span>   <span class="number">12.500000</span>   <span class="number">18.100000</span>    <span class="number">0.000000</span>    <span class="number">0.624000</span></span><br><span class="line"><span class="built_in">max</span>     <span class="number">88.976200</span>  <span class="number">100.000000</span>   <span class="number">27.740000</span>    <span class="number">1.000000</span>    <span class="number">0.871000</span></span><br></pre></td></tr></table></figure>
<h1 id="唯一值函数"><a href="#唯一值函数" class="headerlink" title="唯一值函数"></a>唯一值函数</h1><p>value_counts()可以统计指定列中的值以及值出现的次数。在下面的例子中统计“name”列中出现的值以及出现值的次数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = pd.read_csv(<span class="string">&quot;data2.csv&quot;</span>)</span><br><span class="line">df2[<span class="string">&#x27;name&#x27;</span>].value_counts()</span><br><span class="line"></span><br><span class="line">HK          <span class="number">2</span></span><br><span class="line">Beijing     <span class="number">2</span></span><br><span class="line">Shanghai    <span class="number">1</span></span><br><span class="line">Macau       <span class="number">1</span></span><br><span class="line">Name: name, dtype: int64</span><br></pre></td></tr></table></figure>
<h1 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h1><h2 id="查找缺失值"><a href="#查找缺失值" class="headerlink" title="查找缺失值"></a>查找缺失值</h2><p>isnull()判断数据每一列中是否有空值，有则返回True，没有则返回False。isnull().sum()统计每一列中的空值数量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.isnull().<span class="built_in">sum</span>()</span><br><span class="line">CRIM       <span class="number">20</span></span><br><span class="line">ZN         <span class="number">20</span></span><br><span class="line">INDUS      <span class="number">20</span></span><br><span class="line">CHAS       <span class="number">20</span></span><br><span class="line">NOX         <span class="number">0</span></span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure>
<h2 id="删除nan值并重置序号"><a href="#删除nan值并重置序号" class="headerlink" title="删除nan值并重置序号"></a>删除nan值并重置序号</h2><p>以文件data.csv为例，展示以下几种缺失值处理方法。读取data.csv可见有多处缺失值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1 = pd.read_csv(<span class="string">&quot;data.csv&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df1)</span><br><span class="line">A     B     C    D</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80.0</span>  NaN</span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75.0</span> -<span class="number">1.0</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   NaN  <span class="number">75.0</span>  <span class="number">0.0</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>   NaN -<span class="number">1.0</span></span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80.0</span>  <span class="number">1.0</span></span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>   NaN  NaN</span><br></pre></td></tr></table></figure>
<p>dropna()删除所有含空值的行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1.dropna()</span><br><span class="line"></span><br><span class="line">A	B	C	D</span><br><span class="line"><span class="number">1</span>	<span class="number">4</span>	<span class="number">5.6</span>	<span class="number">75.0</span>	-<span class="number">1.0</span></span><br><span class="line"><span class="number">4</span>	<span class="number">6</span>	<span class="number">3.5</span>	<span class="number">80.0</span>	<span class="number">1.0</span></span><br></pre></td></tr></table></figure>
<p>reset_index()对删除空值后的行重置序号,参数drop&#x3D;True删除原来的index。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">	A	B	C	D</span><br><span class="line"><span class="number">0</span>	<span class="number">4</span>	<span class="number">5.6</span>	<span class="number">75.0</span>	-<span class="number">1.0</span></span><br><span class="line"><span class="number">1</span>	<span class="number">6</span>	<span class="number">3.5</span>	<span class="number">80.0</span>	<span class="number">1.0</span></span><br></pre></td></tr></table></figure>
<h2 id="缺失值填充"><a href="#缺失值填充" class="headerlink" title="缺失值填充"></a>缺失值填充</h2><h3 id="均值填充"><a href="#均值填充" class="headerlink" title="均值填充"></a>均值填充</h3><p>采用df.replace或fillna()将缺失值处替换为均值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#采用replace将D列的缺失值替换为平均值</span></span><br><span class="line">df1.replace(&#123;<span class="string">&#x27;D&#x27;</span>:np.nan&#125;,np.mean(df1[<span class="string">&#x27;D&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">   A     B     C     D</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80.0</span> -<span class="number">0.25</span></span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75.0</span> -<span class="number">1.00</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   NaN  <span class="number">75.0</span>  <span class="number">0.00</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>   NaN -<span class="number">1.00</span></span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80.0</span>  <span class="number">1.00</span></span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>   NaN -<span class="number">0.25</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#采用fillna将D列的缺失值替换为平均值</span></span><br><span class="line">df1[<span class="string">&#x27;D&#x27;</span>].fillna(df1[<span class="string">&#x27;D&#x27;</span>].mean(),inplace=<span class="literal">True</span>)</span><br><span class="line">   A     B     C     D</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80.0</span> -<span class="number">0.25</span></span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75.0</span> -<span class="number">1.00</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   NaN  <span class="number">75.0</span>  <span class="number">0.00</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>   NaN -<span class="number">1.00</span></span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80.0</span>  <span class="number">1.00</span></span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>   NaN -<span class="number">0.25</span></span><br></pre></td></tr></table></figure>
<h3 id="其他值填充"><a href="#其他值填充" class="headerlink" title="其他值填充"></a>其他值填充</h3><p>用一组数据中出现最多次数的值填充，scipy.stats.mode()统计一组数据中出现次数最多的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> st</span><br><span class="line">mode_val=st.mode(df1[<span class="string">&#x27;D&#x27;</span>])[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">df1.replace(&#123;<span class="string">&#x27;D&#x27;</span>:np.nan&#125;,mode_val)</span><br><span class="line"></span><br><span class="line">   A     B     C    D</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80.0</span> -<span class="number">1.0</span></span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75.0</span> -<span class="number">1.0</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   NaN  <span class="number">75.0</span>  <span class="number">0.0</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>   NaN -<span class="number">1.0</span></span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80.0</span>  <span class="number">1.0</span></span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>   NaN -<span class="number">1.0</span></span><br></pre></td></tr></table></figure>
<h3 id="回归预测填充"><a href="#回归预测填充" class="headerlink" title="回归预测填充"></a>回归预测填充</h3><p>对于缺失值为连续性数据，数据之间存在依赖关系的值，可以采用根据其他列的数据，带入线性回归等有监督学习方法来估计缺失值的方法进行缺失值填充。示例采用线性回归模型对D列的缺失值进行填充。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = pd.read_csv(<span class="string">&quot;data2.csv&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df2)</span><br><span class="line"></span><br><span class="line">   A     B   C    D      name</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80</span>  NaN        HK</span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75</span> -<span class="number">1.0</span>  Shanghai</span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   <span class="number">7.0</span>  <span class="number">75</span>  <span class="number">0.0</span>        HK</span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>  <span class="number">80</span> -<span class="number">1.0</span>   Beijing</span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80</span>  <span class="number">1.0</span>   Beijing</span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>  <span class="number">75</span>  NaN     Macau</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model<span class="comment">#导入线性回归工具包</span></span><br><span class="line">df_train = df2.dropna()</span><br><span class="line">x = df_train.drop(columns=[<span class="string">&#x27;D&#x27;</span>,<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line">reg = linear_model.LinearRegression()<span class="comment">#训练拟合模型</span></span><br><span class="line">reg.fit(x,df_train[<span class="string">&#x27;D&#x27;</span>])</span><br><span class="line">predict_d = reg.predict(df2.drop(columns=[<span class="string">&#x27;D&#x27;</span>,<span class="string">&#x27;name&#x27;</span>]))<span class="comment">#预测新的D列</span></span><br><span class="line">df2[<span class="string">&#x27;D&#x27;</span>] = predict_d<span class="comment">#回归预测填充后的数据</span></span><br><span class="line"></span><br><span class="line">   A     B   C             D      name</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80</span>  <span class="number">3.193548e+00</span>        HK</span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75</span> -<span class="number">1.000000e+00</span>  Shanghai</span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   <span class="number">7.0</span>  <span class="number">75</span> -<span class="number">7.105427e-15</span>        HK</span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>  <span class="number">80</span> -<span class="number">1.000000e+00</span>   Beijing</span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80</span>  <span class="number">1.000000e+00</span>   Beijing</span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>  <span class="number">75</span>  <span class="number">1.129032e+00</span>     Macau</span><br></pre></td></tr></table></figure>
<h3 id="hotdeck填充"><a href="#hotdeck填充" class="headerlink" title="hotdeck填充"></a>hotdeck填充</h3><p>将缺失值替换为在一列中随机选择的值。采用ffill方法，将空值填补为上一行中出现的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.fillna(method=<span class="string">&#x27;ffill&#x27;</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h1 id="分类标签数值化"><a href="#分类标签数值化" class="headerlink" title="分类标签数值化"></a>分类标签数值化</h1><h2 id="序数编码"><a href="#序数编码" class="headerlink" title="序数编码"></a>序数编码</h2><p>采用cat.codes将非数值标签进行简单序数编码，例如将Beijing编码为0，HK编码为1，Macau编码为2.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.name = pd.Categorical(df[<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line">df[<span class="string">&#x27;code&#x27;</span>] = df.name.cat.codes</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"></span><br><span class="line">   A     B   C    D      name  code</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">10.5</span>  <span class="number">80</span>  NaN        HK     <span class="number">1</span></span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5.6</span>  <span class="number">75</span> -<span class="number">1.0</span>  Shanghai     <span class="number">3</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>   <span class="number">7.0</span>  <span class="number">75</span>  <span class="number">0.0</span>        HK     <span class="number">1</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>   <span class="number">9.4</span>  <span class="number">80</span> -<span class="number">1.0</span>   Beijing     <span class="number">0</span></span><br><span class="line"><span class="number">4</span>  <span class="number">6</span>   <span class="number">3.5</span>  <span class="number">80</span>  <span class="number">1.0</span>   Beijing     <span class="number">0</span></span><br><span class="line"><span class="number">5</span>  <span class="number">2</span>   <span class="number">8.3</span>  <span class="number">75</span>  <span class="number">1.0</span>     Macau     <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h2><p>one-hot编码将每一个分类类别编码为唯一的二进制向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.get_dummies(df.name)</span><br><span class="line"></span><br><span class="line">   Beijing  HK  Macau  Shanghai</span><br><span class="line"><span class="number">0</span>        <span class="number">0</span>   <span class="number">1</span>      <span class="number">0</span>         <span class="number">0</span></span><br><span class="line"><span class="number">1</span>        <span class="number">0</span>   <span class="number">0</span>      <span class="number">0</span>         <span class="number">1</span></span><br><span class="line"><span class="number">2</span>        <span class="number">0</span>   <span class="number">1</span>      <span class="number">0</span>         <span class="number">0</span></span><br><span class="line"><span class="number">3</span>        <span class="number">1</span>   <span class="number">0</span>      <span class="number">0</span>         <span class="number">0</span></span><br><span class="line"><span class="number">4</span>        <span class="number">1</span>   <span class="number">0</span>      <span class="number">0</span>         <span class="number">0</span></span><br><span class="line"><span class="number">5</span>        <span class="number">0</span>   <span class="number">0</span>      <span class="number">1</span>         <span class="number">0</span></span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title>在云服务器上安装hadoop</title>
    <url>/2023/01/20/Untitled%201/</url>
    <content><![CDATA[<h2 id="Secure-Virtual-Machine-Setup"><a href="#Secure-Virtual-Machine-Setup" class="headerlink" title="Secure Virtual Machine Setup"></a>Secure Virtual Machine Setup</h2><h3 id="set-up-a-virtual-machine"><a href="#set-up-a-virtual-machine" class="headerlink" title="set up a virtual machine"></a>set up a virtual machine</h3><p>Use google cloud-&gt;compute engine-&gt;virtual machine instance set up a virtual</p>
<p>Machine info: n2-standard-2 Intel Cascade Lake 100GB</p>
<p>System: Ubuntu-18.04 </p>
<p>![截屏2023-01-20 下午10.26.13](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Documents&#x2F;截屏2023-01-20 下午10.26.13.png)</p>
<h3 id="set-up-a-firewall"><a href="#set-up-a-firewall" class="headerlink" title="set up a firewall"></a>set up a firewall</h3><p>Use VPC-&gt;firevall set a firewall set the source ip to ‘137.189.0.0&#x2F;16’, that only CUHK can access to the vm.</p>
<p>![截屏2023-01-20 下午10.35.29](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-20 下午10.35.29.png)</p>
<h3 id="set-up-a-SSH-1"><a href="#set-up-a-SSH-1" class="headerlink" title="set up a SSH^[1]^"></a>set up a SSH^[1]^</h3><p>Generate a key pair at local machine</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -f id_rsa</span><br></pre></td></tr></table></figure>

<p>upload the public-key to the VM, and use the secret key to access to the VM.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">shiyuzhuo@LoBP key % ssh -i id_rsa shiyuzhuo@34.92.6.155 </span><br></pre></td></tr></table></figure>

<h1 id="Hadoop-Cluster-Setup"><a href="#Hadoop-Cluster-Setup" class="headerlink" title="Hadoop Cluster Setup"></a>Hadoop Cluster Setup</h1><h2 id="Single-node-Hadoop-Setup"><a href="#Single-node-Hadoop-Setup" class="headerlink" title="Single-node Hadoop Setup"></a>Single-node Hadoop Setup</h2><h3 id="add-a-Hadoop-user"><a href="#add-a-Hadoop-user" class="headerlink" title="add a Hadoop user"></a>add a Hadoop user</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">shiyuzhuo@singlenode:~$ sudo useradd -m hadoop -s /bin/bash</span><br><span class="line">shiyuzhuo@singlenode:~$ sudo passwd hadoop</span><br><span class="line">shiyuzhuo@singlenode:~$ sudo adduser hadoop sudo</span><br><span class="line">shiyuzhuo@singlenode:~$ su hadoop</span><br></pre></td></tr></table></figure>

<h3 id="insatll-SSH"><a href="#insatll-SSH" class="headerlink" title="insatll SSH"></a>insatll SSH</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:~$ sudo apt-get install openssh-server</span><br><span class="line">hadoop@singlenode:~$ ssh localhost</span><br><span class="line">hadoop@singlenode:~$ cd ~/.ssh/ </span><br><span class="line">hadoop@singlenode:~/.ssh$ ssh-keygen -t rsa</span><br><span class="line">hadoop@singlenode:~/.ssh$ cat ./id_rsa.pub &gt;&gt; ./authorized_keys</span><br><span class="line">hadoop@singlenode:~/.ssh$ cd ~</span><br><span class="line">hadoop@singlenode:~$ ssh localhost</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="insatll-JAVA"><a href="#insatll-JAVA" class="headerlink" title="insatll JAVA"></a>insatll JAVA</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:~$ sudo apt-get update</span><br><span class="line">hadoop@singlenode:~$ sudo apt-get install openjdk-8-jre</span><br><span class="line">hadoop@singlenode:~$ sudo apt-get install openjdk-8-jdk</span><br><span class="line">hadoop@singlenode:~$ vim ~/.bashrc</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line">hadoop@singlenode:~$ source  ~/.bashrc</span><br><span class="line">hadoop@singlenode:~$ java -version</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="install-hadoop"><a href="#install-hadoop" class="headerlink" title="install hadoop"></a>install hadoop</h3><p>Download Hadoop and give the user Hadoop permission</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:~$ wget https://archive.apache.org/dist/hadoop/core/hadoop-2.9.2/hadoop-2.9.2.tar.gz</span><br><span class="line">hadoop@singlenode:~$ sudo tar -zxf hadoop-2.9.2.tar.gz -C /usr/local</span><br><span class="line">hadoop@singlenode:~$ cd /usr/local</span><br><span class="line">hadoop@singlenode:/usr/local$ sudo mv ./hadoop-2.9.2 ./hadoop</span><br><span class="line">hadoop@singlenode:/usr/local$ sudo chown -R hadoop ./hadoop</span><br></pre></td></tr></table></figure>

<p>add Java path and launch the Hadoop service</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:/usr/local$ cd hadoop</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ sudo vi etc/hadoop/hadoop-env.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">set</span> to the root of your Java installation</span></span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ bin/hadoop</span><br></pre></td></tr></table></figure>

<p>set up a single-node Hadoop cluster in a pseudo-distributed mode,change the configuration^[3]^</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:/usr/local/hadoop$ sudo vi ./etc/hadoop/core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ sudo vi ./etc/hadoop/hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>configure the environment</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:~$ sudo vi /home/hadoop/.bashrc</span><br><span class="line">export HADOOP_PREFIX=/usr/local/hadoop</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_COMMON_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_HDFS_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export YARN_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$&#123;HADOOP_PREFIX&#125;/lib/native</span><br><span class="line">export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_PREFIX/lib&quot;</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line">export JRE_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre</span><br><span class="line">export PATH=$PATH:/usr/lib/jvm/java-8-openjdk-amd64/bin:/usr/lib/jvm/java-8-openjdk-amd64/jre/bin</span><br></pre></td></tr></table></figure>

<p>Start the service</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:~$ cd /usr/local/hadoop</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ ./bin/hdfs namenode -format</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ ./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>use jps check the single node statement</p>
<h3 id="截屏2023-01-21-上午2-28-19-x2F-Users-x2F-shiyuzhuo-x2F-Library-x2F-Application-Support-x2F-typora-user-images-x2F-截屏2023-01-21-上午2-28-19-png"><a href="#截屏2023-01-21-上午2-28-19-x2F-Users-x2F-shiyuzhuo-x2F-Library-x2F-Application-Support-x2F-typora-user-images-x2F-截屏2023-01-21-上午2-28-19-png" class="headerlink" title="![截屏2023-01-21 上午2.28.19](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午2.28.19.png)"></a>![截屏2023-01-21 上午2.28.19](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午2.28.19.png)</h3><p>visit 34.92.6.155:50070 to access to the hadoop webpage</p>
<p>![3481674134925_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3481674134925_.pic_hd.jpg)</p>
<p>![3491674134925_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3491674134925_.pic_hd.jpg)</p>
<p>The signle node hadoop setup successfully</p>
<h3 id="run-the-Terasort-example-4"><a href="#run-the-Terasort-example-4" class="headerlink" title="run the Terasort example^[4]^"></a>run the Terasort example^[4]^</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@singlenode:/usr/local/hadoop$  ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar teragen 100000 terasort/input</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar terasort terasort/input terasort/output</span><br><span class="line">hadoop@singlenode:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar teravalidate terasort/output terasort/check</span><br></pre></td></tr></table></figure>

<p>![3501674135120_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3501674135120_.pic_hd.jpg)</p>
<p>![3511674135154_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3511674135154_.pic_hd.jpg)</p>
<p>![3521674135172_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3521674135172_.pic_hd.jpg)</p>
<p>![3541674135210_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3541674135210_.pic_hd.jpg)</p>
<h2 id="Multi-node-Hadoop-Cluster-Setup"><a href="#Multi-node-Hadoop-Cluster-Setup" class="headerlink" title="Multi-node Hadoop Cluster Setup"></a>Multi-node Hadoop Cluster Setup</h2><h3 id="set-up-4-instance-on-the-cloud-VM"><a href="#set-up-4-instance-on-the-cloud-VM" class="headerlink" title="set up 4 instance on the cloud VM"></a>set up 4 instance on the cloud VM</h3><p>One is master and the other are slave1,slave2 and slave3. All of them have the same settings</p>
<h2 id="截屏2023-01-21-上午2-40-56-x2F-Users-x2F-shiyuzhuo-x2F-Library-x2F-Application-Support-x2F-typora-user-images-x2F-截屏2023-01-21-上午2-40-56-png"><a href="#截屏2023-01-21-上午2-40-56-x2F-Users-x2F-shiyuzhuo-x2F-Library-x2F-Application-Support-x2F-typora-user-images-x2F-截屏2023-01-21-上午2-40-56-png" class="headerlink" title="![截屏2023-01-21 上午2.40.56](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午2.40.56.png)"></a>![截屏2023-01-21 上午2.40.56](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午2.40.56.png)</h2><p>Set up a hadoop user and modify the SSH configure</p>
<figure class="highlight shell"><figcaption><span>l</span></figcaption><table><tr><td class="code"><pre><span class="line">shiyuzhuo@master:~$ sudo -i</span><br><span class="line">root@master:~# adduser hadoop</span><br><span class="line">root@master:~# vi /etc/ssh/sshd_config</span><br><span class="line">root@master:~# service sshd restart</span><br></pre></td></tr></table></figure>

<p>change the PasswordAuthentication yes</p>
<p>and the ChallengResponseAuthentication yes</p>
<h3 id="set-up-SSH-in-the-cluster"><a href="#set-up-SSH-in-the-cluster" class="headerlink" title="set up SSH in the cluster"></a>set up SSH in the cluster</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@master:~# sudo vi /etc/hosts</span><br><span class="line">10.170.0.2 master.asia-east2-a.c.multi-node2.internal master  </span><br><span class="line">10.170.0.3 slave1.asia-east2-a.c.multi-node2.internal slave1</span><br><span class="line">10.170.0.4 slave2.asia-east2-a.c.multi-node2.internal slave2</span><br><span class="line">10.170.0.5 slave3.asia-east2-a.c.multi-node2.internal slave3</span><br></pre></td></tr></table></figure>

<p>repeat the above commad on the master and 3 slaves.Then generate key pair</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ ssh-keygen -t rsa -P &quot;&quot;</span><br><span class="line">hadoop@master:~$ cat ./.ssh/id_rsa.pub &gt;&gt; ./authorized_keys</span><br><span class="line">hadoop@master:~$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@master</span><br><span class="line">hadoop@master:~$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@slave1</span><br><span class="line">hadoop@master:~$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@slave2</span><br><span class="line">hadoop@master:~$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@slave3</span><br><span class="line">chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<p>test if master can use ssh to connect slaves</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ ssh slave1</span><br></pre></td></tr></table></figure>

<p>![截屏2023-01-21 上午3.17.43](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午3.17.43.png)</p>
<p>successfully set up connections</p>
<h3 id="install-java-and-hadoop-on-the-master"><a href="#install-java-and-hadoop-on-the-master" class="headerlink" title="install java and hadoop on the master"></a>install java and hadoop on the master</h3><p>Just as the steps in the Q0, install java and hadoop and configure the environment.</p>
<h3 id="configure-hadoop"><a href="#configure-hadoop" class="headerlink" title="configure hadoop"></a>configure hadoop</h3><p>New the flod of data node and namenode</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ sudo mkdir -p /usr/local/hadoop_store/tmp </span><br><span class="line">hadoop@master:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode </span><br><span class="line">hadoop@master:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode </span><br><span class="line">hadoop@master:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/secondarynamenode </span><br></pre></td></tr></table></figure>

<p>configure the hadoop documents</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:/usr/local/hadoop/etc/hadoop$ vi hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    	&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    	&lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop_store/hdfs/namenode&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop_store/hdfs/datanode&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop_store/hdfs/secondarynamenode&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3600&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">hadoop@master:/usr/local/hadoop/etc/hadoop$ vi core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/usr/local/hadoop_store/tmp&lt;/value&gt;</span><br><span class="line">    	&lt;description&gt;A base for other temporary directories.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://master:54310&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">hadoop@master:/usr/local/hadoop/etc/hadoop$ vi mapred-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapred.job.tracker&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:54311&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">hadoop@master:/usr/local/hadoop/etc/hadoop$ vi yarn-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">hadoop@slave1:~$ chown -R hadoop:hadoop /usr/local/hadoop</span><br><span class="line">hadoop@slave1:~$ chown -R hadoop:hadoop /usr/local/hadoop_store</span><br></pre></td></tr></table></figure>

<p>Add the slves to the slave file</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ sudo vi /usr/local/hadoop/etc/hadoop/slaves</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">localhost</span></span><br><span class="line">master</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br><span class="line">slave3</span><br></pre></td></tr></table></figure>

<h3 id="copy-the-document-from-master-to-slaves"><a href="#copy-the-document-from-master-to-slaves" class="headerlink" title="copy the document from master to slaves"></a>copy the document from master to slaves</h3><p>Use SCP copy the hadoop settings to slaves. Firstly install java on the three slaves as the steps in Q0, the copy the hadoop stein’s to slaves</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ scp -r /usr/local/hadoop slave1:/home/hadoop/ </span><br><span class="line">hadoop@master:~$ scp -r /usr/local/hadoop_store slave1:/home/hadoop/</span><br><span class="line">hadoop@master:~$ scp -r /home/hadoop/.bashrc slave1:/home/hadoop/</span><br></pre></td></tr></table></figure>

<p>Repeat the commad 3 times, copy the hadoop to the 3 slaves. Then change the catalog and permission of hadoop on each slaves</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@slave3:~$ sudo mv /home/hadoop/hadoop /usr/local/</span><br><span class="line">hadoop@slave3:~$ sudo mv /home/hadoop/hadoop_store /usr/local/</span><br><span class="line">hadoop@slave3:~$ chown -R hadoop:hadoop /usr/local/hadoop</span><br><span class="line">hadoop@slave3:~$ chown -R hadoop:hadoop /usr/local/hadoop_store</span><br></pre></td></tr></table></figure>

<h3 id="Start-the-multi-node-hadoop-cluster"><a href="#Start-the-multi-node-hadoop-cluster" class="headerlink" title="Start the multi-node hadoop cluster"></a>Start the multi-node hadoop cluster</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ hadoop namenode -format</span><br><span class="line">hadoop@master:~$ start-dfs.sh</span><br><span class="line">hadoop@master:~$ start-yarn.sh</span><br></pre></td></tr></table></figure>

<p>use ups check the status</p>
<p>on master</p>
<p>![截屏2023-01-21 上午3.43.06](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午3.43.06.png)</p>
<p>On slaves</p>
<p>![截屏2023-01-21 上午3.43.35](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午3.43.35.png)</p>
<p>visit the web page</p>
<p>![3591674217838_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3591674217838_.pic_hd.jpg)</p>
<p>Successfully start the service.</p>
<h3 id="generate-2-different-datasets-of-size-2GB-and-20GB-to-serve-as-input-for-the-Terasort-program-Run-the-Terasort-code-again-for-these-different-datasets-and-compare-their-running-time"><a href="#generate-2-different-datasets-of-size-2GB-and-20GB-to-serve-as-input-for-the-Terasort-program-Run-the-Terasort-code-again-for-these-different-datasets-and-compare-their-running-time" class="headerlink" title="generate 2 different datasets of size 2GB and 20GB to serve as input for the Terasort program. Run the Terasort code again for these different datasets and compare their running time."></a>generate 2 different datasets of size 2GB and 20GB to serve as input for the Terasort program. Run the Terasort code again for these different datasets and compare their running time.</h3><p>Just like the steps in a, firstly run the 2GB dataset</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar teragen 20000000 terasort/input2GB </span><br><span class="line">hadoop@master:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar terasort terasort/input2GB terasort/output2GB </span><br><span class="line">hadoop@master:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar teravalidate terasort/output2GB terasort/check2GB </span><br></pre></td></tr></table></figure>

<p>Run the 20GB</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar teragen 200000000 terasort/input20GB </span><br><span class="line">hadoop@master:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar terasort terasort/input2GB terasort/output20GB </span><br><span class="line">hadoop@master:/usr/local/hadoop$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar teravalidate terasort/output2GB terasort/check20GB </span><br></pre></td></tr></table></figure>

<p>visit the 34.96.212.237:8088 to monitor the runtime</p>
<p>![截屏2023-01-21 上午3.52.47](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Documents&#x2F;截屏2023-01-21 上午3.52.47.png)</p>
<p>the 2GB runtime: 131 seconds</p>
<p>the 20GB runtime:13755 seconds</p>
<h2 id="Running-the-Python-Code-on-Hadoop"><a href="#Running-the-Python-Code-on-Hadoop" class="headerlink" title="Running the Python Code on Hadoop"></a>Running the Python Code on Hadoop</h2><h3 id="new-the-mapper-py-and-the-reducer-py"><a href="#new-the-mapper-py-and-the-reducer-py" class="headerlink" title="new the mapper.py and the reducer.py"></a>new the mapper.py and the reducer.py</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;mapper.py&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># input comes from STDIN (standard input)</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    <span class="comment"># remove leading and trailing whitespace</span></span><br><span class="line">    line = line.strip()</span><br><span class="line">    <span class="comment"># split the line into words</span></span><br><span class="line">    words = line.split()</span><br><span class="line">    <span class="comment"># increase counters</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="comment"># write the results to STDOUT (standard output);</span></span><br><span class="line">        <span class="comment"># what we output here will be the input for the</span></span><br><span class="line">        <span class="comment"># Reduce step, i.e. the input for reducer.py</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># tab-delimited; the trivial word count is 1</span></span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;%s\t%s&#x27;</span> % (word, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;reducer.py&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">current_word = <span class="literal">None</span></span><br><span class="line">current_count = <span class="number">0</span></span><br><span class="line">word = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># input comes from STDIN</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    <span class="comment"># remove leading and trailing whitespace</span></span><br><span class="line">    line = line.strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse the input we got from mapper.py</span></span><br><span class="line">    word, count = line.split(<span class="string">&#x27;\t&#x27;</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert count (currently a string) to int</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        count = <span class="built_in">int</span>(count)</span><br><span class="line">    <span class="keyword">except</span> ValueError:</span><br><span class="line">        <span class="comment"># count was not a number, so silently</span></span><br><span class="line">        <span class="comment"># ignore/discard this line</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># this IF-switch only works because Hadoop sorts map output</span></span><br><span class="line">    <span class="comment"># by key (here: word) before it is passed to the reducer</span></span><br><span class="line">    <span class="keyword">if</span> current_word == word:</span><br><span class="line">        current_count += count</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> current_word:</span><br><span class="line">            <span class="comment"># write result to STDOUT</span></span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&#x27;%s\t%s&#x27;</span> % (current_word, current_count))</span><br><span class="line">        current_count = count</span><br><span class="line">        current_word = word</span><br><span class="line"></span><br><span class="line"><span class="comment"># do not forget to output the last word if needed!</span></span><br><span class="line"><span class="keyword">if</span> current_word == word:</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&#x27;%s\t%s&#x27;</span> % (current_word, current_count))</span><br></pre></td></tr></table></figure>

<h3 id="download-the-dataset-and-upload-the-dataset-to-the-VM"><a href="#download-the-dataset-and-upload-the-dataset-to-the-VM" class="headerlink" title="download the dataset and upload the dataset to the VM"></a>download the dataset and upload the dataset to the VM</h3><p>download the shakespeare dataset, then upload</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">shiyuzhuo@LoBP key3 % scp -i id_rsa /Users/shiyuzhuo/Documents/shakespeare hadoop@34.96.212.237:~</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>upload the data to HDFS</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ hadoop dfs -copyFromLocal /home/hadoop/shakespeare /user/hadoop/shakespeare</span><br></pre></td></tr></table></figure>

<p>start the map reduce </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar -file /home/hduser/mapper.py -mapper /home/hduser/mapper.py -file /home/hduser/reducer.py -reducer /home/hduser/reducer.py -input /user/hduser/shakespeare -output /user/hduser/shakespeare-output</span><br></pre></td></tr></table></figure>

<p>Check the yarn page</p>
<p>![3651674228851_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3651674228851_.pic_hd.jpg)</p>
<p>successfully run the map reduce job</p>
<p>![截屏2023-01-21 上午4.07.03](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午4.07.03.png)</p>
<p>download the result from HDFS to local to see the output result data</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~/shakespeare-output$ hadoop dfs -copyToLocal /user/hadoop/shakespeare-output3 /home/hadoop</span><br><span class="line">hadoop@master:~/shakespeare-output3$ vi part-00000</span><br></pre></td></tr></table></figure>

<p>The map reduce result</p>
<p>![3631674228731_.pic_hd](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Containers&#x2F;com.tencent.xinWeChat&#x2F;Data&#x2F;Library&#x2F;Application Support&#x2F;com.tencent.xinWeChat&#x2F;2.0b4.0.9&#x2F;76c0f5853654d28cda7dd416e2727d25&#x2F;Message&#x2F;MessageTemp&#x2F;9e20f478899dc29eb19741386f9343c8&#x2F;Image&#x2F;3631674228731_.pic_hd.jpg)</p>
<p>Total running time is 54240(ms)</p>
<h2 id="Compiling-the-Java-WordCount-program-for-MapReduce"><a href="#Compiling-the-Java-WordCount-program-for-MapReduce" class="headerlink" title="Compiling the Java WordCount program for MapReduce"></a>Compiling the Java WordCount program for MapReduce</h2><h3 id="use-“hadoop-classpath”-command-to-get-all-Hadoop-jars"><a href="#use-“hadoop-classpath”-command-to-get-all-Hadoop-jars" class="headerlink" title="use “hadoop classpath” command to get all Hadoop jars."></a>use “hadoop classpath” command to get all Hadoop jars.</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ echo &#x27;export HADOOP_CLASSPATH=$&#123;JAVA_HOME&#125;/lib/tools.jar&#x27; &gt;&gt; /home/hadoop/.bashrc</span><br><span class="line">hadoop@master:~$ source /home/hadoop/.bashrc</span><br></pre></td></tr></table></figure>

<h3 id="new-and-save-the-java-file-8"><a href="#new-and-save-the-java-file-8" class="headerlink" title="new and save the .java file^[8]^"></a>new and save the .java file^[8]^</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop<span class="meta">@master</span>:~$ vi WordCount.java</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCount</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TokenizerMapper</span></span><br><span class="line">       <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">      <span class="type">StringTokenizer</span> <span class="variable">itr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString());</span><br><span class="line">      <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">        word.set(itr.nextToken());</span><br><span class="line">        context.write(word, one);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">IntSumReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text,IntWritable,Text,IntWritable&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">      <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">        sum += val.get();</span><br><span class="line">      &#125;</span><br><span class="line">      result.set(sum);</span><br><span class="line">      context.write(key, result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;word count&quot;</span>);</span><br><span class="line">    job.setJarByClass(WordCount.class);</span><br><span class="line">    job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">    job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">    job.setReducerClass(IntSumReducer.class);</span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">    System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="create-a-jar-and-run-the-jar"><a href="#create-a-jar-and-run-the-jar" class="headerlink" title="create a jar and run the jar"></a>create a jar and run the jar</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop@master:~$ hadoop com.sun.tools.javac.Main WordCount.java</span><br><span class="line">hadoop@master:~$ jar cf wc.jar WordCount*.class</span><br><span class="line">hadoop@master:~$ hadoop jar wc.jar WordCount /user/hadoop/shakespeare /user/hadoop/shakespeare-output-java</span><br></pre></td></tr></table></figure>

<p>![截屏2023-01-21 上午10.58.25](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-01-21 上午10.58.25.png)</p>
<p>Total running time is 14386(ms), faster than the python program</p>
]]></content>
  </entry>
  <entry>
    <title>算法与数据结构-链表</title>
    <url>/2023/02/07/Untitled%202/</url>
    <content><![CDATA[<p>链表是一种通过指针串联在一起的线性结构，每一个节点由两部分组成，一个是数据域一个是指针域（存放指向下一个节点的指针），最后一个节点的指针域指向null（空指针的意思）。</p>
<p>链表的入口节点称为链表的头结点也就是head。</p>
<span id="more"></span>

<h1 id="链表的存储方式"><a href="#链表的存储方式" class="headerlink" title="链表的存储方式"></a>链表的存储方式</h1><p>了解完链表的类型，再来说一说链表在内存中的存储方式。</p>
<p>数组是在内存中是连续分布的，但是链表在内存中可不是连续分布的。</p>
<p>链表是通过指针域的指针链接在内存中各个节点。</p>
<p>所以链表中的节点在内存中不是连续分布的 ，而是散乱分布在内存中的某地址上，分配机制取决于操作系统的内存管理。</p>
<h1 id="链表定义"><a href="#链表定义" class="headerlink" title="链表定义"></a>链表定义</h1><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Listnode</span> &#123;</span><br><span class="line"><span class="type">int</span> val;</span><br><span class="line">ListNode *next;</span><br><span class="line"><span class="built_in">LisNode</span>(<span class="type">int</span> x) : <span class="built_in">val</span>(x),<span class="built_in">next</span>(<span class="literal">NULL</span>)&#123;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h2 id="通过自己定义构造函数初始化节点："><a href="#通过自己定义构造函数初始化节点：" class="headerlink" title="通过自己定义构造函数初始化节点："></a>通过自己定义构造函数初始化节点：</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">ListNode* head = <span class="keyword">new</span> <span class="built_in">ListNode</span>(<span class="number">5</span>);</span><br></pre></td></tr></table></figure>

<h2 id="使用默认构造函数初始化节点："><a href="#使用默认构造函数初始化节点：" class="headerlink" title="使用默认构造函数初始化节点："></a>使用默认构造函数初始化节点：</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">ListNode* head = <span class="keyword">new</span> <span class="built_in">ListNode</span>();</span><br><span class="line">head-&gt;val = <span class="number">5</span>;</span><br></pre></td></tr></table></figure>

<p>所以如果不定义构造函数使用默认构造函数的话，在初始化的时候就不能直接给变量赋值！</p>
<h1 id="链表操作"><a href="#链表操作" class="headerlink" title="链表操作"></a>链表操作</h1><h1 id="反转链表"><a href="#反转链表" class="headerlink" title="反转链表"></a>反转链表</h1><p>双指针法，一个指针指向头节点，另外一个指针为null</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">public:</span><br><span class="line">    ListNode* <span class="title function_">reverseList</span><span class="params">(ListNode* head)</span> &#123;</span><br><span class="line">        ListNode* tmp;</span><br><span class="line">        ListNode* pre = nullptr;</span><br><span class="line">        ListNode* cur = head;</span><br><span class="line">        <span class="keyword">while</span>(cur != nullptr)&#123;</span><br><span class="line">          <span class="comment">//保存下一个节点，为遍历下一个节点赋值</span></span><br><span class="line">            tmp = cur-&gt;next;</span><br><span class="line">          <span class="comment">//反转链表</span></span><br><span class="line">            cur-&gt;next = pre;</span><br><span class="line">          <span class="comment">//更新pre指针</span></span><br><span class="line">            pre = cur;</span><br><span class="line">          <span class="comment">//更新cur指针</span></span><br><span class="line">            cur = tmp;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> pre;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="链表相交"><a href="#链表相交" class="headerlink" title="链表相交"></a>链表相交</h1><p>给你两个单链表的头节点 <code>headA</code> 和 <code>headB</code> ，请你找出并返回两个单链表相交的起始节点。如果两个链表不存在相交节点，返回 <code>null</code> 。</p>
<p>哈希法，用哈希表存储a链表中的节点，再去b里查找，如果找到了就返回第一个查找到的节点。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode *<span class="title">getIntersectionNode</span><span class="params">(ListNode *headA, ListNode *headB)</span> </span>&#123;</span><br><span class="line">        unordered_set&lt;ListNode *&gt; res;</span><br><span class="line">        ListNode *t = headA;</span><br><span class="line">        <span class="keyword">while</span>(t != <span class="literal">NULL</span>)&#123;</span><br><span class="line">            res.<span class="built_in">insert</span>(t);</span><br><span class="line">            t = t-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        t = headB;</span><br><span class="line">        <span class="keyword">while</span>(t != <span class="literal">NULL</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(res.<span class="built_in">count</span>(t))&#123;</span><br><span class="line">                <span class="keyword">return</span> t;</span><br><span class="line">            &#125;</span><br><span class="line">            t = t-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h1 id="回文链表"><a href="#回文链表" class="headerlink" title="回文链表"></a>回文链表</h1><p>给你一个单链表的头节点 <code>head</code> ，请你判断该链表是否为回文链表。如果是，返回 <code>true</code> ；否则，返回 <code>false</code> 。</p>
<p>将链表的值保存到数组中，用双指针从数组两端分别遍历查询，查询到不一样的则返回false</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isPalindrome</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line">        <span class="keyword">while</span>(head != <span class="literal">nullptr</span>)&#123;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(head -&gt; val);</span><br><span class="line">            head = head -&gt; next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>,j = ans.<span class="built_in">size</span>() - <span class="number">1</span>;i &lt;= j;i++,j--)&#123;</span><br><span class="line">            <span class="keyword">if</span>(ans[i] != ans[j])&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h1 id="环形链表"><a href="#环形链表" class="headerlink" title="环形链表"></a>环形链表</h1><h2 id="判断是否有环"><a href="#判断是否有环" class="headerlink" title="判断是否有环"></a>判断是否有环</h2><p>快慢指针法，慢指针一次前进一步，快指针一次前进两步，如果两个指针相遇，则链表有环</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">hasCycle</span><span class="params">(ListNode *head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(head == <span class="literal">NULL</span> || head -&gt; next == <span class="literal">NULL</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode *slow = head;</span><br><span class="line">        ListNode *fast = head -&gt; next;</span><br><span class="line">        <span class="keyword">while</span>(slow != fast)&#123;</span><br><span class="line">            <span class="keyword">if</span>(fast == <span class="literal">NULL</span> || fast -&gt; next == <span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            slow = slow -&gt; next;</span><br><span class="line">            fast = fast -&gt; next -&gt; next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h2 id="判断环的入口"><a href="#判断环的入口" class="headerlink" title="判断环的入口"></a>判断环的入口</h2><p>三指针，首先快慢指针同时出发，当快慢指针相遇时，再让一个新的指针从头节点出发，新指针与慢指针相遇时，即为环的入口。</p>
<p>原理:慢指针走过的距离a+b，快指针走过的距离a+b+c+b，快指针的路程是慢指针的两倍</p>
<p>2a + 2b &#x3D; a+b+c+b 故 a &#x3D; c</p>
<p>![截屏2023-07-07 上午11.29.11](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;截屏2023-07-07 上午11.29.11.png)</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode *<span class="title">detectCycle</span><span class="params">(ListNode *head)</span> </span>&#123;</span><br><span class="line">        ListNode *fast = head;</span><br><span class="line">        ListNode *slow = head;</span><br><span class="line">        <span class="keyword">while</span>(fast != <span class="literal">NULL</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(fast == <span class="literal">NULL</span> || fast -&gt; next == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">            fast = fast -&gt; next -&gt; next;</span><br><span class="line">            slow = slow -&gt; next;</span><br><span class="line">            <span class="keyword">if</span>(fast == slow)&#123;</span><br><span class="line">                ListNode *t = head;</span><br><span class="line">                <span class="keyword">while</span>(t != slow)&#123;</span><br><span class="line">                    t = t -&gt; next;</span><br><span class="line">                    slow = slow -&gt; next;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> t;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h1 id="链表排序"><a href="#链表排序" class="headerlink" title="链表排序"></a>链表排序</h1><h1 id="两数相加"><a href="#两数相加" class="headerlink" title="两数相加"></a>两数相加</h1><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">addTwoNumbers</span><span class="params">(ListNode* l1, ListNode* l2)</span> </span>&#123;</span><br><span class="line">        ListNode *head = <span class="literal">nullptr</span>,*tail = <span class="literal">nullptr</span>;</span><br><span class="line">        <span class="type">int</span> carry = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(l1 || l2)&#123;</span><br><span class="line">            <span class="type">int</span> n1 = l1 ? l1 -&gt; val : <span class="number">0</span>;</span><br><span class="line">            <span class="type">int</span> n2 = l2 ? l2 -&gt; val : <span class="number">0</span>;</span><br><span class="line">            <span class="type">int</span> sum = n1 + n2 + carry;</span><br><span class="line">            <span class="keyword">if</span>(!head)&#123;</span><br><span class="line">                head = tail = <span class="keyword">new</span> <span class="built_in">ListNode</span>(sum%<span class="number">10</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                tail -&gt; next = <span class="keyword">new</span> <span class="built_in">ListNode</span>(sum%<span class="number">10</span>);</span><br><span class="line">                tail = tail -&gt; next;</span><br><span class="line">            &#125;</span><br><span class="line">            carry = sum / <span class="number">10</span>;</span><br><span class="line">            <span class="keyword">if</span>(l1)&#123;</span><br><span class="line">                l1 = l1 -&gt; next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(l2)&#123;</span><br><span class="line">                l2 = l2 -&gt; next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(carry &gt; <span class="number">0</span>)&#123;</span><br><span class="line">                tail -&gt; next= <span class="keyword">new</span> <span class="built_in">ListNode</span>(carry);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>





<h1 id="删除链表中倒数第n个节点"><a href="#删除链表中倒数第n个节点" class="headerlink" title="删除链表中倒数第n个节点"></a>删除链表中倒数第n个节点</h1><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">removeNthFromEnd</span><span class="params">(ListNode* head, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        ListNode *dhead = <span class="keyword">new</span> <span class="built_in">ListNode</span>(<span class="number">0</span>);</span><br><span class="line">        dhead -&gt; next = head;</span><br><span class="line">        ListNode *l = dhead,*r = dhead;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt;= n;i++)&#123;</span><br><span class="line">            r = r -&gt; next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(r != <span class="literal">nullptr</span>)&#123;</span><br><span class="line">            l = l -&gt; next;</span><br><span class="line">            r = r -&gt; next;</span><br><span class="line">        &#125;</span><br><span class="line">        l -&gt; next = l -&gt; next -&gt; next;</span><br><span class="line">        <span class="keyword">return</span> dhead -&gt; next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h1 id="两两交换链表中的节点"><a href="#两两交换链表中的节点" class="headerlink" title="两两交换链表中的节点"></a>两两交换链表中的节点</h1><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">swapPairs</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(head == <span class="literal">nullptr</span> || head -&gt; next == <span class="literal">nullptr</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> head;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode *nhead = head -&gt; next;</span><br><span class="line">        head -&gt; next = <span class="built_in">swapPairs</span>(nhead -&gt; next);</span><br><span class="line">        nhead -&gt; next = head;</span><br><span class="line">        <span class="keyword">return</span> nhead;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

]]></content>
  </entry>
  <entry>
    <title>AB测试</title>
    <url>/2023/06/12/abtest/</url>
    <content><![CDATA[<p>AB测试是为明确某个问题，制作两个（A&#x2F;B）或多个（A&#x2F;B&#x2F;n）版本，在同一时间维度，分别让组成成分相同（相似）的访客群组（目标人群）随机的访问这些版本，收集各群组的用户体验数据和业务数据；最后分析、评估出最好版本，正式采用。</p>
<span id="more"></span>

<h1 id="实验流程"><a href="#实验流程" class="headerlink" title="实验流程"></a>实验流程</h1><h2 id="提出需求"><a href="#提出需求" class="headerlink" title="提出需求"></a>提出需求</h2><p>设置实验组，实验组新算法A，对照组原算法B；配置uv分桶，各50%流量，<strong>确定分析目标</strong></p>
<h2 id="实验开发"><a href="#实验开发" class="headerlink" title="实验开发"></a>实验开发</h2><p>数据埋点；指标集成；用户分流</p>
<h2 id="结论分析"><a href="#结论分析" class="headerlink" title="结论分析"></a>结论分析</h2><p>检查错误；确定置信区间</p>
<h2 id="要点："><a href="#要点：" class="headerlink" title="要点："></a>要点：</h2><p>1、时间一致性；</p>
<p>2、数据分布的一致性；</p>
<p>3、统计显著的结果才可以引导决策；</p>
<p>4、实验分组设计上（流量分布要均匀）：</p>
<p>算法对用户的偏差没有反映到实验分桶上，会放大算法之间效果的差距，从而产生辛普森悖论；</p>
<p><img src="/2023/06/12/abtest/img1.png"></p>
<p><img src="/2023/06/12/abtest/img2.png"></p>
<p> 5、置信</p>
<p> 要获得一个可信的试验结果需要一定的流量（样本）和时间，如果流量（样本）太小或者分不均匀，试验结果会存在偶然性，可能无法得出可信的结果；试验运行时间太短的话同理；</p>
<p>6、时间</p>
<p>实验周期中也要避免外部因素的影响，尽量在平稳时期进行，减少外部因素的干扰；有时候为了保证实验效果的置信，防止小流量分布不均匀，可以在试验过程中，逐步增大流量分配，同时监控关键指标的数据走势，从而得到置信的结论；</p>
<h1 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h1><p>对总体的参数提出假设，利用样本的信息去判断假设是否成立</p>
<p>原假设：新版本可以提高点击率；备择假设：新版本不能提高点击率</p>
<p><strong>而假设检验就是先对总体的参数提出某种假设，然后利用样本的信息判断假设是否成立的过程</strong>，比如上面的假设信息我该接受还是拒绝。</p>
<p><img src="/2023/06/12/abtest/img3.png"></p>
<h2 id="二类错误"><a href="#二类错误" class="headerlink" title="二类错误"></a>二类错误</h2><p>一类错误：弃真，拒绝原假设</p>
<p>二类错误：取伪，</p>
<p>弃真：我的实验改动实际没有用，但我认为它有用</p>
<p>纳伪：我的实验改动实际有用，但我认为它没有用</p>
<p>在统计学中，我们<strong>用显著性水平（α）来衡量犯第一类错误的概率</strong>，统计功效β来衡量犯第二类错误的概率</p>
<p>检验统计量：<strong>即计算检验的统计量。根据给定的显著性水平，查表得出相应的临界值。再将检验统计量的值与该显著性水平的临界值进行比较</strong>，得出是否拒绝原假设的结论。</p>
<p><img src="/2023/06/12/abtest/img4.png"></p>
]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>/2022/12/12/cnn/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>线性回归推导及实现</title>
    <url>/2022/07/19/cs229n/</url>
    <content><![CDATA[<p>有监督学习，目的是构建一种算法，来完成$ h:x\rightarrow y $的映射，$ x^i $为输入特征，$ y^i $ 为预测类别，${(x^i,y^i);i&#x3D;1…n}$构成训练集。当$ y $ 为连续值时，该问题是一种回归问题，映射$h(x)$为与真实值$y$接近的预测值。最简单的有监督学习的回归问题是线性回归。</p>
<span id="more"></span>

<h1 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h1><p>  以预测房屋价格来描述一个线性回归问题。以房屋价格数据集为例，数据集包含房屋的大小与价格。</p>
<table>
<thead>
<tr>
<th>大小size</th>
<th>价格price</th>
</tr>
</thead>
<tbody><tr>
<td>2104</td>
<td>400</td>
</tr>
<tr>
<td>1600</td>
<td>330</td>
</tr>
<tr>
<td>2400</td>
<td>369</td>
</tr>
<tr>
<td>1416</td>
<td>232</td>
</tr>
</tbody></table>
<p>在这一问题中，房屋的价格可以通过房屋的大小进行预测，</p>
<p> ![Untitled Diagram.drawio](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Downloads&#x2F;Untitled Diagram.drawio.png)<br>其中假设$h(x)$表示为$h(x)&#x3D;\theta_0+\theta_1x $,，$x$为房屋的大小，$h(x)$为预测得到的房屋价格。当把问题进行补充，将输入特征增加为居住面积与卧室数目，这样输入特征$x$就变为一个二维向量。为简化多维向量的回归方程的表达，$h(x)$可以表示为<br>$$<br>h(x)&#x3D;\sum_{j &#x3D; 0}{n}\theta_jx_j&#x3D;\theta^Tx<br>$$</p>
<h1 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h1><p>当预测值$h(x)$越趋进于真实值$y$时，回归方程的预测效果越好，为了度量预测值与真实值之间的距离，采用最小二乘函数$J(\theta)$又称作MSE或L2损失函数。<br>$$<br>J(\theta) &#x3D; \frac{1}{2}\sum_{i &#x3D; 0} ^m(y^i - h_\theta (x^i))^2<br>$$</p>
<p>是回归方程达到最优值的方法即为使损失函数最小化。即<br>$$<br>min_{\theta}J(\theta) &#x3D; \frac{1}{2}\sum_{i &#x3D; 0} ^m(y^i - h_\theta (x^i))^2<br>$$<br>下面给出参数$\theta$的数值解法与梯度下降解法</p>
<h1 id="参数求解"><a href="#参数求解" class="headerlink" title="参数求解"></a>参数求解</h1><p>如果最小二乘函数$J(\theta)$存在全局最小值，那么存在$\hat{\theta}$使得它对于$\theta$的偏导数为0即<br>$$<br>\nabla_{\theta}J(\hat\theta))&#x3D;0<br>$$<br>求解过程如下<br>$$<br>J(\theta) &#x3D; \frac{1}{2}\sum_{i &#x3D; 0} ^m(y^i - h_\theta (x^i))^2&#x3D;\frac{1}{2}\sum_{i &#x3D; 0} ^m(\theta^Tx^i-y^i)^2<br>$$<br>根据矩阵的计算改写$J(\theta)$<br>$$<br>\begin{aligned}<br>J(\theta)<br>&amp; &#x3D; \frac{1}{2}\sum_{i &#x3D; 0} ^m(\theta^Tx^i-y^i)^T(\theta^Tx^i-y^i) \<br>&amp; &#x3D; \frac{1}{2}(\theta^Tx^Tx\theta-y\theta^Tx^T-y^Tx\theta+y^Ty)\<br>&amp; &#x3D; \frac{1}{2}(\theta^Tx^Tx\theta-2\theta^Tx^Ty+y^Ty)<br>\end{aligned}<br>$$</p>
<p>$$<br>\hat{\theta}&#x3D;argminJ(\theta)<br>$$</p>
<p>对$\theta$求偏导，导数为0时为极小值点，令$\frac{\partial J(\theta)}{\partial\theta}&#x3D;0$求得参数$\theta$的值。<br>$$<br>\frac{\partial J(\theta)}{\partial\theta}&#x3D;\frac{1}{2}(2x^Tx\theta-2x^Ty)&#x3D;0<br>$$<br>参数$\theta$的表达式为<br>$$<br>\theta&#x3D;(x^Tx)^{-1}x^Ty<br>$$</p>
<h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><p>对于使得最小二乘损失函数最优化的方法，采用<strong>梯度下降</strong>方法，梯度下降法通过不断在目标函数的梯度方向上修正参数来达到最优解。梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度），即<br>$$<br>\theta_j:&#x3D;\theta_j-\alpha\frac{\partial J(\theta)}{\partial\theta_j}<br>$$<br>$\alpha$为学习率。迭代过程为：1、选取初始值$\theta$   2、在梯度方向上修正$\theta$   3、直到$\theta_j$收敛至局部最优</p>
<h2 id="梯度下降的推导"><a href="#梯度下降的推导" class="headerlink" title="梯度下降的推导"></a>梯度下降的推导</h2><p>$$<br>\begin{aligned}<br>\frac{\partial J(\theta)}{\partial\theta_j}<br>&amp; &#x3D; (h_\theta(x)-y) \frac{\partial}{\partial\theta_j}（h_\theta(x)-y) \<br>&amp; &#x3D; (h_\theta(x)-y) \frac{\partial}{\partial\theta_j}(\sum_{j&#x3D;0}^n\theta_jx_j-y) \<br>&amp; &#x3D; (h_\theta(x)-y)x_j<br>\end{aligned}<br>$$</p>
<h1 id="线性回归的sklearn实现"><a href="#线性回归的sklearn实现" class="headerlink" title="线性回归的sklearn实现"></a>线性回归的sklearn实现</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data1=pd.read_csv(<span class="string">&#x27;ex1data1.txt&#x27;</span>,header=<span class="literal">None</span>)</span><br><span class="line">data1.head()</span><br><span class="line"></span><br><span class="line">x = data1.iloc[:,<span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line">y = data1.iloc[:,<span class="number">1</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">data1.info()</span><br><span class="line">plt.scatter(x,y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">clf = LinearRegression()</span><br><span class="line">pred = clf.fit(x,y)</span><br><span class="line"><span class="built_in">print</span>(clf.coef_)<span class="comment">#系数</span></span><br><span class="line"><span class="built_in">print</span>(clf.intercept_)<span class="comment">#截距</span></span><br><span class="line">yy = clf.predict(x)</span><br><span class="line">plt.figure(figsize = (<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line">plt.scatter(x,y,label = <span class="string">&#x27;train_data&#x27;</span>)</span><br><span class="line">plt.plot(x,yy,<span class="string">&#x27;r&#x27;</span>,label = <span class="string">&#x27;linear_regression&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()`</span><br></pre></td></tr></table></figure>

<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/07/15/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>HQL性能调优</title>
    <url>/2022/07/17/hql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</url>
    <content><![CDATA[<h1 id="hive简介"><a href="#hive简介" class="headerlink" title="hive简介"></a>hive简介</h1><p>Hive是基于Hadoop的数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单sql查询功能，可以将sql语句转换为MapReduce任务进行运行。<br>sql命令-&gt;HIVE处理，转换为MapReduce-&gt;提交任务到Hadoop,HDFS,MapReduce运行</p>
<h1 id="hive-ddl"><a href="#hive-ddl" class="headerlink" title="hive-ddl"></a>hive-ddl</h1><h3 id="hive建表（压缩表和非压缩表）"><a href="#hive建表（压缩表和非压缩表）" class="headerlink" title="hive建表（压缩表和非压缩表）"></a>hive建表（压缩表和非压缩表）</h3><p>一个表有一个或多个分区，每个分区以文件夹的形式单独存放在表文件夹的目录下</p>
<p>创建表，指定EXTERNAL是外部表，没有指定内部表，内部表在drop时会从HDFS上删除数据，外部表不会删除。</p>
<p>如果不指定数据库，hive把表创建在default数据库下</p>
<p>内部表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> login(</span><br><span class="line">userid <span class="type">bigint</span>,</span><br><span class="line">ip string</span><br><span class="line"><span class="type">time</span> <span class="type">bigint</span>)</span><br><span class="line"><span class="keyword">partition</span> <span class="keyword">by</span>(dt string)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure>
<p>外部表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">location</span><br></pre></td></tr></table></figure>
<h1 id="hive-dml"><a href="#hive-dml" class="headerlink" title="hive-dml"></a>hive-dml</h1><h3 id="hive优化的根本思想"><a href="#hive优化的根本思想" class="headerlink" title="hive优化的根本思想"></a>hive优化的根本思想</h3><p>尽早过滤数据，减少每个阶段的数据量<br>减少job数<br>解决数据倾斜问题</p>
<h3 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h3><p><strong>1、列裁剪</strong><br>只选择需要的字段<br><strong>2、分区裁剪</strong><br>查询过程中减少不必要的分区</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(orderid)</span><br><span class="line"><span class="keyword">from</span> order_table</span><br><span class="line"><span class="keyword">where</span> to_date(sale_time)<span class="operator">=</span><span class="string">&#x27;2014-03-03&#x27;</span> <span class="keyword">and</span></span><br><span class="line"><span class="keyword">hour</span>(to_date(sale_time))<span class="operator">=</span><span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>改为</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(orderid)</span><br><span class="line"><span class="keyword">from</span> order_table</span><br><span class="line"><span class="keyword">where</span> dt<span class="operator">=</span><span class="string">&#x27;2014-03-03&#x27;</span> <span class="keyword">and</span> to_date(sale_time)<span class="operator">=</span><span class="string">&#x27;2014-03-03&#x27;</span> <span class="keyword">and</span></span><br><span class="line"><span class="keyword">hour</span>(to_date(sale_time))<span class="operator">=</span><span class="number">10</span></span><br></pre></td></tr></table></figure>
<p><code>explain dependency</code>语法获取扫描的分区<br>**3、利用hive优化机制减少job数 **<br>三个以上表关联时，如果join的key相同，都会合并成一个mapreduce任务</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select a.val,b.val,c.val</span><br><span class="line">from a join b on(a.key=b.key1)</span><br><span class="line">join c on (c.key=b.key1)</span><br><span class="line">#优化为一个job</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>HIVE</category>
      </categories>
  </entry>
  <entry>
    <title>在谷歌云平台搭建kubernetes</title>
    <url>/2023/03/15/k8s/</url>
    <content><![CDATA[<p>kubernetes是一个可扩展的管理容器化服务的平台，本文将演示如何在谷歌云平台搭建k8s</p>
<span id="more"></span>

<h1 id="搭建单节点k8s集群"><a href="#搭建单节点k8s集群" class="headerlink" title="搭建单节点k8s集群"></a>搭建单节点k8s集群</h1><p>在 GCP上新建一个虚拟机</p>
<p><img src="/2023/03/15/k8s/img1.png"></p>
<p>设置防火墙规则，仅允许cuhk段ip访问。</p>
<p>本地设置密钥对，远程连接到服务器。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh -i id_rsa shiyuzhuo@34.92.43.56  </span><br></pre></td></tr></table></figure>

<p>下载安装docker</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# sudo apt-get update</span><br><span class="line">root@instancek8s:~# sudo apt-get install ca-certificates curl gnupg lsb-release</span><br><span class="line">root@instancek8s:~# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg</span><br><span class="line">root@instancek8s:~# echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu (lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br><span class="line">root@instancek8s:~# sudo apt-get update</span><br><span class="line">root@instancek8s:~# sudo apt-get install docker-ce docker-ce-cli containerd.io -y</span><br></pre></td></tr></table></figure>

<p>下载安装 kubedam kubectl 和 kubelet</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# sudo apt-get update</span><br><span class="line">root@instancek8s:~# sudo apt-get install -y apt-transport-https ca-certificates curl</span><br><span class="line">root@instancek8s:~# sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg</span><br><span class="line">root@instancek8s:~# echo &quot;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">root@instancek8s:~# sudo apt-get update</span><br><span class="line">root@instancek8s:~# sudo apt-get install -y kubelet kubeadm kubectl</span><br><span class="line">root@instancek8s:~# sudo apt-mark hold kubelet kubeadm kubectl</span><br></pre></td></tr></table></figure>

<p>配置k8s</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# sudo ufw disable</span><br><span class="line">root@instancek8s:~# sudo swapoff -a; sed -i &#x27;/swap/d&#x27; /etc/fstab</span><br><span class="line">root@instancek8s:~# sudo systemctl daemon-reload</span><br><span class="line">root@instancek8s:~# sudo systemctl restart docker</span><br><span class="line">root@instancek8s:~# sudo systemctl restart kubelet</span><br><span class="line">root@instancek8s:~# cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf</span><br><span class="line">br_netfilter</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">root@instancek8s:~# cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">root@instancek8s:~# sudo sysctl --system</span><br><span class="line">root@instancek8s:~# echo 1 &gt; /proc/sys/net/ipv4/ip_forward</span><br></pre></td></tr></table></figure>

<p>初始化</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# rm /etc/containerd/config.toml</span><br><span class="line">root@instancek8s:~# systemctl restart containerd</span><br><span class="line">root@instancek8s:~# kubeadm init</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/15/k8s/img2.png"></p>
<p>设置节点为master，在节点上部署pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line">root@instancek8s:~# wget https://docs.projectcalico.org/v3.25/manifests/calico.yaml --no-check-certificate</span><br><span class="line">root@instancek8s:~# kubectl apply -f calico.yaml</span><br></pre></td></tr></table></figure>

<p>删除 taint</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# kubectl taint nodes --all node-role.kubernetes.io/control-plane-</span><br></pre></td></tr></table></figure>

<p>部署hello world yaml</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# kubectl apply -f https://mobitec.ie.cuhk.edu.hk/iems5730Spring2023/static_files/assignments/hello-world-demo.yaml</span><br></pre></td></tr></table></figure>

<p>检查pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# kubectl get pods</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/15/k8s/img3.png"></p>
<p>访问30123端口</p>
<p><img src="/2023/03/15/k8s/img4.png"></p>
<h1 id="Kubernetes-集群搭建"><a href="#Kubernetes-集群搭建" class="headerlink" title="Kubernetes 集群搭建"></a>Kubernetes 集群搭建</h1><p>在 GCP上按照相同的配置设置四台虚拟机</p>
<p><img src="/2023/03/15/k8s/img5.png"></p>
<p>与上一步相同，下载与安装docker，kubedam kubectl 和 kubelet</p>
<p>初始化k8s， 在mater节点部署pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# kubeadm init</span><br><span class="line">root@instancek8s:~# export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line">root@instancek8s:~# wget https://docs.projectcalico.org/v3.25/manifests/calico.yaml --no-check-certificate</span><br><span class="line">root@instancek8s:~# kubectl apply -f calico.yaml</span><br></pre></td></tr></table></figure>

<p>在集群中加入其他节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@k8smaster:~# kubeadm token create --print-join-command</span><br><span class="line">kubeadm join 10.170.0.5:6443 --token 98hfcy.wcggumrd54d79zku --discovery-token-ca-cert-hash sha256:3450e1a3523d39a20f5b6e2e2ee94a003cb3537d2a47fbf7f28b6fff07eaa660 </span><br></pre></td></tr></table></figure>

<p>在其他三个节点运行join command</p>
<p><img src="/2023/03/15/k8s/img6.png"></p>
<p><img src="/2023/03/15/k8s/img7.png"></p>
<p><img src="/2023/03/15/k8s/img8.png"></p>
<p><img src="/2023/03/15/k8s/img9.png"></p>
<p>在master节点上部署hadoop</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@k8smaster:~# kubectl create -f https://mobitec.ie.cuhk.edu.hk/iems5730Spring2023/static_files/assignments/hadoop.yaml</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/15/k8s/img10.png"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@k8smaster:~# kubectl get pods</span><br><span class="line">root@k8smaster:~# kubectl get service</span><br><span class="line">root@k8smaster:~# kubectl get pods -o wide</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/15/k8s/img11.png"></p>
<h1 id="试用谷歌云Serverless搭建Kubernetes-服务"><a href="#试用谷歌云Serverless搭建Kubernetes-服务" class="headerlink" title="试用谷歌云Serverless搭建Kubernetes 服务"></a>试用谷歌云Serverless搭建Kubernetes 服务</h1><p>在 GKE上建立一个k8s集群</p>
<p><img src="/2023/03/15/k8s/img12.png"></p>
<p>连接到集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gcloud container clusters get-credentials cluster-1 --zone asia-east1-a --project multi-node-375214</span><br></pre></td></tr></table></figure>

<p>部署hadoop.yaml </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl create -f https://mobitec.ie.cuhk.edu.hk/iems5730Spring2023/static_files/assignments/hadoop.yaml</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/15/k8s/img13.png"></p>
<p>访问端口32007</p>
<p><img src="/2023/03/15/k8s/img14.png"></p>
<p>运行Hadoop Terasort</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl exec --stdin --tty hdfs-master -- /bin/bash</span><br><span class="line"> cd /usr/local/hadoop</span><br><span class="line"></span><br><span class="line"> ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teragen 20000000 terasort/input2GB</span><br><span class="line"> ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar terasort terasort/input2GB terasort/output2GB</span><br><span class="line"> ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teravalidate terasort/output2GB terasort/check2GB</span><br><span class="line"></span><br><span class="line"> ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teragen 200000000 terasort/input20GB </span><br><span class="line"> ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar terasort terasort/input20GB terasort/output20GB</span><br><span class="line"> ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar teravalidate terasort/output20GB terasort/check20GB </span><br></pre></td></tr></table></figure>

<p> ![截屏2023-02-28 下午8.57.18](&#x2F;Users&#x2F;shiyuzhuo&#x2F;dwxa&#x2F;source&#x2F;_posts&#x2F;k8s&#x2F;截屏2023-02-28 下午8.57.18.png)</p>
<p>![截屏2023-02-28 下午8.57.42](&#x2F;Users&#x2F;shiyuzhuo&#x2F;Documents&#x2F;截屏2023-02-28 下午8.57.42.png)</p>
<p>运行时间</p>
<p>2GB:40+269+46 &#x3D; 355s</p>
<p>20GB:739+14839+288 &#x3D; 15866s</p>
<h1 id="Fault-tolerance"><a href="#Fault-tolerance" class="headerlink" title="Fault-tolerance"></a>Fault-tolerance</h1><p>Kill hello-world pods</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@instancek8s:~# kubectl delete pods hello-world-b6mj2</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/15/k8s/img15.png"></p>
<p>kill pods hello-world-b6mj2 之后，一个新的pod hello-world-p4pr6 出现.在第一个pod失败后，将有一个具有相同配置的新pod来完成容错</p>
<p>Reference:</p>
<p>[1]<a href="https://docs.docker.com/engine/install/ubuntu/">https://docs.docker.com/engine/install/ubuntu/</a></p>
<p>[2]<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a></p>
<p>[3]<a href="https://blog.csdn.net/qq_43580215/article/details/125153959">https://blog.csdn.net/qq_43580215/article/details/125153959</a></p>
<p>[4]<a href="https://stackoverflow.com/questions/51121136/the-connection-to-the-server-localhost8080-was-refused-did-you-specify-the-ri">https://stackoverflow.com/questions/51121136/the-connection-to-the-server-localhost8080-was-refused-did-you-specify-the-ri</a></p>
<p>[5]<a href="https://cloud.tencent.com/document/product/457/42948">https://cloud.tencent.com/document/product/457/42948</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>/2023/04/04/mapreduce/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/2022/10/29/pca/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>raft协议理论</title>
    <url>/2023/02/07/raft/</url>
    <content><![CDATA[<p>raft是一种管理日志复制的共识算法，其比paxos更加易于理解而且为构建实用的系统提供了更好的基础。</p>
<span id="more"></span>

<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>为了提高大规模数据的读写性能，有以下两种构想，一是提高机器本身的性能，提高机器的计算能力与存储能力，第二种则是增加机器的数量，以集群的形式处理大规模数据，但是集群内部机器的网络请求行为也会成为集群性能的瓶颈。</p>
<p>这种分布式系统带来的优势主要体现在能够进行数据备份，避免单节点故障导致的不可用的情况，增加容错，二是多个节点共同执行任务，达到负载均衡的效果。同样，分布式系统也存在一些问题，比如如何保证数据的一致性，与分布式系统的可用性。</p>
<h2 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a>CAP理论</h2><p>cap分别代表一致性，可用性与分区容错性。</p>
<p><img src="/2023/02/07/raft/img1.png"></p>
<p>这三种属性共存的情况不成立，只能三者满足其二。</p>
<p>C： 一致性，强调正确性，要求整个分布式系统像一个不可拆分的系统，写操作作用于集群像作用于单机一样。</p>
<p>A：可用性，强调使用的体验，客户端的请求都能得到响应，不发生错误，也没有过长的等待时间。</p>
<p>P：分区容错性，在网络环境不可靠的情况下，整个系统仍然是正常运作的，不至于出现系统崩溃。</p>
<p>对于分布式系统，为了保证系统正常运行，P是需要得到保证的。</p>
<p>CP系统：强调系统数据的正确性，可能会牺牲可用性。</p>
<p>AP系统：强调系统可用性，会在数据一致性上做妥协。</p>
<p>一致性与可用性的矛盾在于网络环境的不稳定。而raft算法采取的多数派原则摒弃了一部分的一致性，使得系统得以正常运转。</p>
<h1 id="分布式共识算法"><a href="#分布式共识算法" class="headerlink" title="分布式共识算法"></a>分布式共识算法</h1><h1 id="领导者选举"><a href="#领导者选举" class="headerlink" title="领导者选举"></a>领导者选举</h1><h1 id="日志复制"><a href="#日志复制" class="headerlink" title="日志复制"></a>日志复制</h1><h2 id="raft新特点"><a href="#raft新特点" class="headerlink" title="raft新特点"></a>raft新特点</h2><p>stronger leader： raft 采用了更加强大的领导者角色，在日志复制时，仅从领导者流向其他服务器</p>
<p>leader election：raft采用随机的timer来进行leader election，仅通过对心跳增加少量机制就可以快速解决冲突</p>
<p>成员变化：raft对于更改集群中的服务器的机制使用了新的联合共识方法</p>
<p>2 复制状态机问题</p>
<p>3 paxos的优缺点</p>
<p>4 可理解性的一般方法</p>
<h2 id="raft共识算法"><a href="#raft共识算法" class="headerlink" title="raft共识算法"></a>raft共识算法</h2><p>raft实现共识的过程为首先选举一名领导者·，然后让领导者全权负责管理日志复制。leader从客户端接收日志，并将日志复制在其他的服务器上，然后告诉服务器合适将日志应用于状态机是安全的。当leader fail时，选出新的leader。raft将这个共识问题分解为三个相对独立的子问题，即<strong>领导者选举，日志复制，选举安全</strong>。</p>
<p>在raft中每一个服务器都处于以下三种状态，领导人<em><strong>leader</strong></em>，追随者<em><strong>follower</strong></em>，候选人<em><strong>candidate</strong></em>，正常状态下只有一个领导者，负责处理所有来自客户端的请求，追随者不会发出请求，只对领导者和候选人的请求作出回应，如果客户端联系追随者，追随者会重定向到领导者。候选人用于选举领导者。</p>
<p>raft将时间分成任意长度的<em><strong>term</strong></em>，term由连续数字进行编号。每个term由一次选举开始，在选举中一个或多个候选人竞选，如果一个候选人赢得竞选，那么它在term的剩余时间成为领导者，如果投票没有产生领导者，在这种情况下在term结束时没有领导者，这时候一个新的term会马上开始。raft保证在每个term中最多只有一个领导者。</p>
<p><em><strong>term</strong></em>在raft中作为一个逻辑时钟，他们允许服务器检测过时的信息，比如过时的领导者，每个服务器都存储着当前term的编号，随着term的增加而变化。当前term在服务器通信时交换。如果一个服务器的当前term编号小于另一个服务器的，这时这个服务器会更新自己的编号。如果候选人或者领导者发现自己的任期已过，则会恢复到低一级的状态，如果服务器接收到过时term的请求，会拒绝该请求。</p>
<p>raft服务器之间使用rpc进行通信，基本的共识算法采用两种rpc，RequestVote RPCs在候选人选举期间发起，Append-Entries RPCs由领导者发起，来进行日志复制与心跳，</p>
<h2 id="leader-election"><a href="#leader-election" class="headerlink" title="leader election"></a>leader election</h2><p>raft采用心跳机制<em><strong>heartbeats</strong></em>触发领导者选举。</p>
<p>服务启动时，所有节点都为追随者，直到节点收到领导者或候选人明确的rpc时都保持追随者状态。领导人向所有追随者发送阶段性的心跳来维护他的地位。如果追随者在一段时间内<em><strong>electiontimeout</strong></em>没有收到来自领导者的信息，那么就会发起一次新的选举。</p>
<p>开始选举时，追随者增加当前term并转换为候选人状态，然后他将给自己投票并且向集群中的其他节点发出RequestVote RPC。当以下三种情况发生时，结束候选人身份。1、这个节点赢得选举2、其他节点成为领导者3、没有选举出领导者。当一个候选人赢得一个term中的集群中大部分服务器的选票（大于及群众及其数量的二分之一）时，它将赢得选举，每个服务器最多给一位候选人投票。</p>
<p>赢得选举后，发送心跳消息。在候选人等待投票时，可能会收到来自另一台服务器声称自己是领导者的消息，如果领导者的term至少与候选人当前的term相同，这名候选人承认领导者，并返回追随者的状态。如果小于候选者的term，候选者拒绝rpc，并继续保持候选人状态。</p>
<p>如果多名追随者变为候选人，那么就有可能出现没有候选人获得多数的选票。raft使用随机的选举超时<em><strong>electiontimeout</strong></em>来保证这种情况很少发生。首先为了防止选票的分散，将从一个固定的范围内选择选举超时的时间，因此在大多数情况下只有一台服务器会超时，它赢得选举并且在其他服务器超时之前发送心跳。</p>
<p>log replication</p>
<p>9 评估raft</p>
]]></content>
  </entry>
  <entry>
    <title>假设检验</title>
    <url>/2022/07/20/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</url>
    <content><![CDATA[<h2 id="参数估计与假设检验"><a href="#参数估计与假设检验" class="headerlink" title="参数估计与假设检验"></a>参数估计与假设检验</h2><p>参数估计讨论的是用样本估计总体参数的方法，总体参数μ在估计前是未知的。<br>而在假设检验中，则是先对μ的值提出一个假设，然后利用样本信息去检验这个假设是否成立。</p>
<span id="more"></span>

<h2 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h2><p>假设检验：假设就是对从总体参数(均值、比例等)的具体数值所作的陈述，比如，我认为配方一比配方二的效果要好。<strong>而假设检验就是先对总体的参数提出某种假设，然后利用样本的信息判断假设是否成立的过程</strong>，比如上面的假设信息我该接受还是拒绝。</p>
<h2 id="显著性水平"><a href="#显著性水平" class="headerlink" title="显著性水平"></a>显著性水平</h2><p><strong>显著性水平是一个概率值，原假设为真时，拒绝原假设的概率，表示为α，常取值为0.05、0.01、0.10。</strong>一个公司招聘，本来准备招聘100个人，公司希望只有5%的人是混水摸鱼招聘进来，所以可能会有5个人混进来，所谓显著性水平α，就是你允许有多少比例混水摸鱼的能通过测试。</p>
<h2 id="检验统计量"><a href="#检验统计量" class="headerlink" title="检验统计量"></a>检验统计量</h2><p><strong>即计算检验的统计量。根据给定的显著性水平，查表得出相应的临界值。再将检验统计量的值与该显著性水平的临界值进行比较</strong>，得出是否拒绝原假设的结论。</p>
<h2 id="假设检验的两种错误"><a href="#假设检验的两种错误" class="headerlink" title="假设检验的两种错误"></a>假设检验的两种错误</h2><h3 id="弃真"><a href="#弃真" class="headerlink" title="弃真"></a>弃真</h3><p><em><strong>类型 I 错误(弃真)，如原假设为真，但否定它，则会犯类型 I 错误。犯类型 I 错误的概率为 α（即您为假设检验设置的显著性水平）。</strong></em>α 为 0.05 表明，当您否定原假设时，您愿意接受 5% 的犯错概率。</p>
<h3 id="取伪"><a href="#取伪" class="headerlink" title="取伪"></a>取伪</h3><p><em><strong>原假设实际上是不正确的，但是我们却做出了接受原假设的决定，此类错误称为第二类错误</strong></em></p>
<h2 id="单双侧检验"><a href="#单双侧检验" class="headerlink" title="单双侧检验"></a>单双侧检验</h2><p>当假设关键词有不得少于&#x2F;低于的时候用左侧检验，比如灯泡的使用寿命不得少于&#x2F;低于700小时时；当假设关键词有不得多于&#x2F;高于的时候用右侧检验，比如次品率不得多于&#x2F;高于5%时。双侧检验指按分布两端计算显著性水平概率的检验，应用于理论上不能确定两个总体一个一定比另一个大或小的假设检验。</p>
<h2 id="假设检验结果"><a href="#假设检验结果" class="headerlink" title="假设检验结果"></a>假设检验结果</h2><p>一般假设检验写作<em>H0：μ1&#x3D;μ2</em>。<br>单侧，若<em>p值&gt;α</em>,不拒绝H0，若p值&lt;α,拒绝$H_0$；双侧，若<em>p值&gt;1&#x2F;2α</em>,不拒绝$H_0$，若<em>p值&lt;1&#x2F;2α</em>,拒绝$H_0$<br>假设检验方法：z检验，t检验，卡方检验<br>两独立样本t检验(ab实验背后原理)</p>
<h2 id="Q-Q图"><a href="#Q-Q图" class="headerlink" title="Q-Q图"></a>Q-Q图</h2><p>Q-Q图鉴别样本数据是否近似正态分布，样本数据近似落在一条直线附近，则近似于正态分布，直线斜率为<em><strong>标准差</strong></em>，截距为<em><strong>均值</strong></em></p>
]]></content>
      <categories>
        <category>统计学原理</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>/2022/12/12/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>数据探索性分析（EDA）</title>
    <url>/2022/08/30/%E6%95%B0%E6%8D%AEEDA/</url>
    <content><![CDATA[<p>数据探索性分析往往是我们在进行数据建模的第一步，数据的质量也决定着模型效果的上限，因此数据探索性分析也是解决数据科学问题中的重要一环，下面介绍数据探索性分析的简要流程及几种常见处理方法。以阿里云天池学习赛工业蒸汽数据集为例。</p>
<span id="more"></span>

<h1 id="数据信息探索（缺失值，异常值，数据分布）"><a href="#数据信息探索（缺失值，异常值，数据分布）" class="headerlink" title="数据信息探索（缺失值，异常值，数据分布）"></a>数据信息探索（缺失值，异常值，数据分布）</h1><h2 id="数据分布"><a href="#数据分布" class="headerlink" title="数据分布"></a>数据分布</h2><h2 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h2><h2 id="异常值处理"><a href="#异常值处理" class="headerlink" title="异常值处理"></a>异常值处理</h2><h1 id="特征相关性"><a href="#特征相关性" class="headerlink" title="特征相关性"></a>特征相关性</h1><h1 id="多重共线性"><a href="#多重共线性" class="headerlink" title="多重共线性"></a>多重共线性</h1><h1 id="数据降维"><a href="#数据降维" class="headerlink" title="数据降维"></a>数据降维</h1><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><h1 id="归一化处理"><a href="#归一化处理" class="headerlink" title="归一化处理"></a>归一化处理</h1><h2 id="最大最小归一化"><a href="#最大最小归一化" class="headerlink" title="最大最小归一化"></a>最大最小归一化</h2><h2 id="Z-Score归一化"><a href="#Z-Score归一化" class="headerlink" title="Z-Score归一化"></a>Z-Score归一化</h2>]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
  </entry>
  <entry>
    <title>数据集市相关概念（来自aws）</title>
    <url>/2023/02/07/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B8%82/</url>
    <content><![CDATA[<p>最近工作中遇到数据集市相关概念，在aws上看到一篇相关文章，引用学习一下。</p>
<span id="more"></span>

<h2 id="什么是数据集市？"><a href="#什么是数据集市？" class="headerlink" title="什么是数据集市？"></a>什么是数据集市？</h2><p>数据集市是一种数据存储系统，其中包含特定于组织业务部门的信息。它包含公司存储在更大存储系统中的一小部分选定数据。公司使用数据集市更高效地分析特定于部门的信息。它可提供汇总数据，关键利益相关者可以使用这些数据快速制定明智决策。 </p>
<p>例如，某公司可能会在其数据仓库或数据湖中存储来自各种来源的数据，如供应商信息、订单、传感器数据、员工信息和财务记录。但该公司将与营销部门相关的信息（如社交媒体评论和客户记录）存储在数据集市中。</p>
<h2 id="数据集市与其他类型的数据存储系统相比如何？"><a href="#数据集市与其他类型的数据存储系统相比如何？" class="headerlink" title="数据集市与其他类型的数据存储系统相比如何？"></a>数据集市与其他类型的数据存储系统相比如何？</h2><p>很多公司使用几种不同类型的数据存储系统进行数据管理和分析。让我们来看一些常见的数据存储类型，以了解公司使用数据集市的环境。</p>
<h3 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a><strong>数据库</strong></h3><p>数据库是计算机系统用来存储、搜索、检索和分析信息的有组织的存储。数据库有多种类型，如关系数据库。关系数据库将信息存储在由行和列组成的表中。不同表中的数据通过称为键的唯一标识符进行连接。键是特定列中的非重复值。</p>
<h4 id="数据集市与数据库的对比"><a href="#数据集市与数据库的对比" class="headerlink" title="*数据集市与数据库的对比*"></a><em><strong>*数据集市与数据库的对比*</strong></em></h4><p>数据集市充当部门数据的前置元素。 您可以使用数据集市来检索和分析信息。而数据库则可收集、管理和存储信息。然后，您可以使用工具对存储的信息进行处理、格式化并将其传输到数据集市。 </p>
<h3 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a><strong>数据仓库</strong></h3><p><a href="https://aws.amazon.com/data-warehouse/">数据仓库</a>是一个庞大的数据库系统，用于存储整个企业的信息。它从各种来源（如商业软件和社交媒体源）收集原始信息，并将其处理为以表格格式存储的结构化数据。企业可以将企业数据仓库连接到商业智能工具，以制定更明智的决策。 </p>
<h4 id="数据集市与数据仓库的对比"><a href="#数据集市与数据仓库的对比" class="headerlink" title="*数据集市与数据仓库的对比*"></a><em><strong>*数据集市与数据仓库的对比*</strong></em></h4><p>数据集市与数据仓库具有很多共同特质。它们的不同之处在于，数据仓库包含有关各种主题的企业范围的数据。而数据集市则存储与特定主题密切相关的信息。例如，数据仓库可能存储营销、人力资源、采购和客户支持部门的信息。而数据集市可能只存储与单一部门相关的事务数据。建立数据集市的吸引力在于，管理数据集市的部门可以完全控制其数据的加载和管理。 </p>
<p>很多组织目前使用诸如数据共享之类的技术将其数据集市发布到中央数据仓库。 通过这样做，他们可以通过分配所有权和隔离工作负载来提高敏捷性。 同样，数据共享允许部门数据集市使用从数据仓库或其他数据集市共享的数据。</p>
<h3 id="数据湖"><a href="#数据湖" class="headerlink" title="数据湖"></a><strong>数据湖</strong></h3><p><a href="https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/">数据湖</a>是保存原始和非结构化信息的数据存储。它不会将信息存储在文件和文件夹中。相反，它将未经处理的信息存储在海量存储上的扁平层次结构中。数据湖存储不同类型的原始信息，包括文本文档、图像、视频和音频。 </p>
<p>数据分析师使用数据湖对非结构化数据进行预测分析。例如，数据湖可能会存储来自社交媒体评论的文本，企业可以将其用于情绪分析。数据分析师可以使用情绪分析来检测针对某家公司的负面意见趋势。 </p>
<h4 id="数据集市与数据湖的对比"><a href="#数据集市与数据湖的对比" class="headerlink" title="*数据集市与数据湖的对比*"></a><em><strong>*数据集市与数据湖的对比*</strong></em></h4><p>由于数据湖存储未经处理的数据，因此某些信息可能是重复的，或者可能对公司没有意义。而数据集市则存储满足特定需求的经过处理的数据。数据湖可以是数据集市的源。企业通过查看数据集市中的历史数据来确定数据趋势，但它们使用数据湖来深入分析存储的信息。 </p>
<h3 id="OLAP"><a href="#OLAP" class="headerlink" title="OLAP"></a><strong>OLAP</strong></h3><p>在线分析处理 (OLAP) 是以多维度表示数据的方法。例如，数据分析师使用 OLAP 多维数据集同时显示基于月份、城市和产品的销售收入。OLAP 数据结构范围很广，包含分类为事实或维度的字段，从而导致数据重复。 这与传统的关系数据库形成鲜明对比，后者倾向于范围较窄的结构，因此数据重复很少。</p>
<h4 id="数据集市与OLAP-多维数据集的对比"><a href="#数据集市与OLAP-多维数据集的对比" class="headerlink" title="*数据集市与OLAP 多维数据集的对比*"></a><em><strong>*数据集市与OLAP 多维数据集的对比*</strong></em></h4><p>OLAP 是一种特定的信息存储策略，它将数据非规范化为范围广泛的表。OLAP 可以简化多维数据的复杂表示方法。一些数据集市可能使用 OLAP 来结构化其信息，但其他数据集市则使用传统的规范化结构。业务分析师可以从 OLAP 结构中受益，使来自数据集市的信息可视化。 </p>
<h3 id="运营数据存储"><a href="#运营数据存储" class="headerlink" title="运营数据存储"></a><strong>运营数据存储</strong></h3><p>运营数据存储 (ODS) 是充当数据来源与数据仓库之间的中介的信息存储。数据分析师可以使用 ODS 提供有关事务数据的准实时报告。ODS 支持简单查询，并且仅提供有限数量的信息。例如，ODS 可能只存储过去 12 小时的销售记录。 </p>
<h4 id="数据集市与ODS-的对比"><a href="#数据集市与ODS-的对比" class="headerlink" title="*数据集市与ODS 的对比*"></a><em><strong>*数据集市与ODS 的对比*</strong></em></h4><p>数据集市从数据仓库中提取面向主题的信息，而 ODS 则将信息发送到数据仓库中进行处理。数据集市提供可供您分析的历史信息，而 ODS 则提供当前运营的最新视图。例如，您可以使用数据集市来确定上个季度的销售模式，但可以接收来自 ODS 的每小时销售数字更新。 </p>
<h2 id="为什么数据集市非常重要？"><a href="#为什么数据集市非常重要？" class="headerlink" title="为什么数据集市非常重要？"></a>为什么数据集市非常重要？</h2><p>以下是公司可能使用数据集市的一些充分理由。 </p>
<h3 id="更高效地检索数据"><a href="#更高效地检索数据" class="headerlink" title="更高效地检索数据"></a><strong>更高效地检索数据</strong></h3><p>通过使用数据集市，公司可以更高效地访问特定信息。与数据仓库相比，数据集市包含部门经常访问的相关和详细信息。因此，业务经理无需搜索整个数据仓库即可生成绩效报告或图形。</p>
<h3 id="简化决策"><a href="#简化决策" class="headerlink" title="简化决策"></a><strong>简化决策</strong></h3><p>公司可以使用数据集市为数据仓库中的数据创建子集。然后，部门内的员工可以分析数据，并根据同一组信息制定决策。 </p>
<h3 id="更有效地控制信息"><a href="#更有效地控制信息" class="headerlink" title="更有效地控制信息"></a><strong>更有效地控制信息</strong></h3><p>数据集市可为员工提供高度精细的访问权限。这意味着公司可以授权特定人选查看或检索特定数据。它可以帮助公司改善数据治理并强制实施信息访问策略。例如，您可以使用数据集市为员工提供针对数据仓库中特定信息的用户访问权限。</p>
<h3 id="灵活地管理数据"><a href="#灵活地管理数据" class="headerlink" title="灵活地管理数据"></a><strong>灵活地管理数据</strong></h3><p>与数据仓库相比，数据集市更小，包含的表更少。这意味着数据工程师可以管理和更改数据集市中的信息，而不会导致重大的数据库更改。</p>
<h2 id="数据集市的工作原理是什么？"><a href="#数据集市的工作原理是什么？" class="headerlink" title="数据集市的工作原理是什么？"></a>数据集市的工作原理是什么？</h2><p>数据集市会将原始信息转化为结构化、有意义的内容，供特定业务部门使用。为了实现这一目标，数据工程师需要建立一个数据集市，以便从数据仓库或直接从外部数据来源接收信息。 </p>
<p>在将数据集市连接到数据仓库时，数据集市将检索与业务部门相关的精选信息。通常，这些信息包含汇总数据，但不包括不必要或详细的数据。 </p>
<h3 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a><strong>ETL</strong></h3><p>提取、转换、加载 (ETL) 是将来自各种数据来源的信息集成和传输到单一物理数据库中的过程。当信息不是来自数据仓库时，数据集市将使用 ETL 从外部来源检索信息。该过程包括以下步骤。</p>
<ul>
<li>提取：从各种来源收集原始信息</li>
<li>转换：将信息结构化为通用格式</li>
<li>加载：将处理后的数据传输到数据库</li>
</ul>
<p>ETL 工具从外部来源（如电子表格、应用程序和文本文档）复制信息。然后，数据集市以结构化形式处理、组织和存储这些信息。 </p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a><strong>分析</strong></h3><p>业务分析师使用软件工具检索、分析和表示数据集市中的数据。例如，他们将存储在数据集市中的信息用于商业智能分析、报告控制面板和云应用程序。 </p>
<p>每个数据集市为少量用户提供服务。例如，营销经理和高级营销人员有权访问数据集市，因此生成报告和图表或执行预测分析所需的时间更少。</p>
<h2 id="数据集市有哪些类型？"><a href="#数据集市有哪些类型？" class="headerlink" title="数据集市有哪些类型？"></a>数据集市有哪些类型？</h2><p>以下是不同类型的数据集市。 </p>
<h3 id="依赖型数据集市"><a href="#依赖型数据集市" class="headerlink" title="依赖型数据集市"></a><strong>依赖型数据集市</strong></h3><p>依赖型数据集市使用来自集中式数据仓库中的信息的子集填充其存储。数据仓库从多种数据来源收集所有信息。然后，数据集市从数据仓库中查询和检索特定于主题的信息。 </p>
<h4 id="优点和缺点"><a href="#优点和缺点" class="headerlink" title="*优点和缺点*"></a><em><strong>*优点和缺点*</strong></em></h4><p>大部分数据管理工作都是在数据仓库中执行的。这意味着业务分析师不需要精通数据库管理即可使用来自数据集市的信息。尽管依赖型数据集市使检索信息变得更加容易，但它们存在单点故障。如果数据仓库发生故障，所有连接的数据集市也将发生故障。 </p>
<h3 id="独立型数据集市"><a href="#独立型数据集市" class="headerlink" title="独立型数据集市"></a><strong>独立型数据集市</strong></h3><p>独立型数据集市不依赖中央数据仓库或任何其他数据集市。每个数据集市都从其来源收集信息，而不是从数据仓库中收集信息。独立型数据集市适用于规模较小但只有特定部门需要访问和分析信息的公司。</p>
<h4 id="优点和缺点-1"><a href="#优点和缺点-1" class="headerlink" title="*优点和缺点*"></a><em><strong>*优点和缺点*</strong></em></h4><p>公司可以相对轻松地建立独立型数据集市。但管理它们可能比较困难。这是因为业务分析师需要在每个数据集市执行数据库管理工作。使用数据共享等策略在不同的数据集市之间共享数据非常简单；多个部门可以读取另一个部门的数据，甚至可以用它们自己的数据对另一个部门的数据进行扩充。 但是，必须制定强有力的数据编录策略，以确保每个部门都知道自己在查看什么。 </p>
<h3 id="混合型数据集市"><a href="#混合型数据集市" class="headerlink" title="混合型数据集市"></a><strong>混合型数据集市</strong></h3><p>混合型数据集市从数据仓库和外部来源收集信息。这使众多公司能在将数据定向到数据仓库之前，灵活地测试独立数据来源。 </p>
<p>例如，假设您推出了一种新产品，并想分析其初始销售数据。数据集市使用直接来自电子商务软件的销售信息，并从数据集市中检索其他产品的销售记录。在该产品成为您店铺中的永久固定商品后，您即可将交易详细信息传送到数据仓库。</p>
<h2 id="数据集市的结构是什么？"><a href="#数据集市的结构是什么？" class="headerlink" title="数据集市的结构是什么？"></a>数据集市的结构是什么？</h2><p>数据集市使用以下结构来存储和表示信息。 </p>
<h3 id="星型"><a href="#星型" class="headerlink" title="星型"></a><strong>星型</strong></h3><p>星型结构的中心有一个事实表，并分支到多个维度表。这样会产生星形连接。事实表是一个数据表，其中包含可用于分析目的的汇总数据。而维度表则将描述性信息保存在事实表中。每个维度表都使用外键链接到事实表。外键是一种唯一标识符，如产品 ID 或供应商 ID。 </p>
<p>例如，销售交易的事实表包含以下列：</p>
<ul>
<li>销售 ID</li>
<li>产品 ID</li>
<li>供应商 ID</li>
<li>销售金额</li>
</ul>
<p>产品的维度表存储以下信息：</p>
<ul>
<li>产品 ID</li>
<li>产品名称</li>
<li>产品成本</li>
</ul>
<p>供应商维度表包含以下列：</p>
<ul>
<li>供应商 ID</li>
<li>供应商名称</li>
<li>城市</li>
</ul>
<h4 id="优势"><a href="#优势" class="headerlink" title="*优势*"></a><em><strong>*优势*</strong></em></h4><p>在星形结构中，维度表是非规范化的，不会扩展到其他表中。这意味着维度表可能包含冗余数据，但能提高搜索和检索速度。它为了存储维度表而占用的空间更少。</p>
<p>业务分析师可以使用星形结构的数据集市来简化复杂查询。当他们搜索特定销售记录时，数据管理系统会搜索整个事实表。当数据集市系统查找正确的记录时，它将使用产品 ID 和供应商 ID 从相应的维度表中查询数据。 </p>
<h3 id="非规范化型"><a href="#非规范化型" class="headerlink" title="非规范化型"></a><strong>非规范化型</strong></h3><p>非规范化型结构会将所有相关数据存储在单个表中。它在事实表和维度表之间没有复杂的联合。数据分析师使用非规范化型数据集市的原因在于它可以提高查询速度。例如，在单个非规范化型表中搜索销售记录如下所示：</p>
<ul>
<li>销售 ID</li>
<li>产品 </li>
<li>产品名称</li>
<li>产品成本</li>
<li>型号名称</li>
<li>重量 </li>
<li>大小</li>
<li>供应商 </li>
<li>供应商名称</li>
<li>城市</li>
<li>销售金额</li>
</ul>
<p>非规范化型数据集市由于采用单表方法，因此适用于实时报告。但是，数据集市的非规范化会导致数据冗余。例如，同一产品名称可能会出现在多个记录中。这会导致额外的存储空间和昂贵的实施成本。</p>
]]></content>
  </entry>
  <entry>
    <title>常见机器学习算法sklearn实现demo</title>
    <url>/2022/10/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0demo/</url>
    <content><![CDATA[<p>导入相关的库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">from</span> kneed <span class="keyword">import</span> KneeLocator</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br></pre></td></tr></table></figure>

<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>LinearRegression()线性回归，十折交叉验证，计算评价指标MSE</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">problem_5</span>(<span class="params">filename,predictors,target</span>):</span><br><span class="line">    <span class="comment"># write your logic here, model is the MLR model</span></span><br><span class="line">    df = pd.read_csv(filename)</span><br><span class="line">    X = df[predictors].values.astype(<span class="built_in">float</span>)</span><br><span class="line">    y = df[target].values.astype(<span class="built_in">float</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    sc_X = StandardScaler()</span><br><span class="line">    sc_y = StandardScaler()</span><br><span class="line">    X = sc_X.fit_transform(X)</span><br><span class="line">    y = sc_y.fit_transform(y)</span><br><span class="line">    mse = []</span><br><span class="line">    regr = linear_model.LinearRegression()</span><br><span class="line">    kf = KFold(n_splits=<span class="number">10</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">5726</span>)</span><br><span class="line">    <span class="keyword">for</span> X_train, X_test <span class="keyword">in</span> kf.split(X):</span><br><span class="line">        regr.fit(X[X_train], y[X_train])</span><br><span class="line">        y_pred = regr.predict(X[X_test])</span><br><span class="line">        mse.append(mean_squared_error(y[X_test], y_pred))</span><br><span class="line">    mean_cv_mse = np.mean(mse)</span><br><span class="line">    sd_cv_mse = np.std(mse)</span><br><span class="line">    model = regr</span><br><span class="line">    <span class="keyword">return</span> model, mean_cv_mse, sd_cv_mse</span><br></pre></td></tr></table></figure>

<h1 id="随机森林分类"><a href="#随机森林分类" class="headerlink" title="随机森林分类"></a>随机森林分类</h1><p>随机森林分类器，十折交叉验证，评价指标准确率</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">problem_3</span>(<span class="params">filename,predictors,target</span>):</span><br><span class="line">    <span class="comment"># write your logic here, model is the RF model</span></span><br><span class="line">    df = pd.read_csv(filename)</span><br><span class="line">    train_data = df[predictors]</span><br><span class="line">    label = df[target]</span><br><span class="line">    PredictorScaler = StandardScaler()</span><br><span class="line">    PredictorScalerFit = PredictorScaler.fit(train_data)</span><br><span class="line">    x_train = PredictorScalerFit.transform(train_data)</span><br><span class="line">    model = RandomForestClassifier(random_state=<span class="number">5726</span>)</span><br><span class="line">    acc_arr = cross_val_score(model, x_train, label, scoring=<span class="string">&#x27;accuracy&#x27;</span>, cv=<span class="number">10</span>)</span><br><span class="line">    mean_cv_acc = np.mean(acc_arr)</span><br><span class="line">    sd_cv_acc = np.std(acc_arr)</span><br><span class="line">    <span class="keyword">return</span> model, mean_cv_acc, sd_cv_acc</span><br></pre></td></tr></table></figure>

<h1 id="支持向量回归"><a href="#支持向量回归" class="headerlink" title="支持向量回归"></a>支持向量回归</h1><p>评价指标MAE,RMSE</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">problem_4</span>(<span class="params">filename,predictors,target</span>):</span><br><span class="line">    <span class="comment"># write your logic here, model is the SVR model</span></span><br><span class="line">    df = pd.read_csv(filename)</span><br><span class="line">    X = df[predictors].values.astype(<span class="built_in">float</span>)</span><br><span class="line">    y = df[target].values.astype(<span class="built_in">float</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    sc_X = StandardScaler()</span><br><span class="line">    sc_y = StandardScaler()</span><br><span class="line">    X = sc_X.fit_transform(X)</span><br><span class="line">    y = sc_y.fit_transform(y)</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>,</span><br><span class="line">                                                        random_state=<span class="number">5726</span>)</span><br><span class="line">    regressor = SVR(kernel=<span class="string">&#x27;poly&#x27;</span>)</span><br><span class="line">    regressor.fit(X_train, y_train)</span><br><span class="line">    y_pred = regressor.predict(X_test)</span><br><span class="line">    test_mae = mean_absolute_error(y_test, y_pred)</span><br><span class="line">    test_rmse = mean_squared_error(y_test, y_pred) ** <span class="number">0.5</span></span><br><span class="line">    model = regressor</span><br><span class="line">    <span class="keyword">return</span> model, test_mae, test_rmse</span><br></pre></td></tr></table></figure>

<h1 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">problem_6</span>(<span class="params">train_filename,predictors,test_filename</span>):</span><br><span class="line">    <span class="comment"># write your logic here, model is the k-mean model</span></span><br><span class="line">    df_train = pd.read_csv(train_filename)</span><br><span class="line">    df_test = pd.read_csv(test_filename)</span><br><span class="line">    train_data = df_train[predictors]</span><br><span class="line">    test_data = df_test[predictors]</span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    scaled_features = scaler.fit_transform(train_data)</span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    scaled_features2 = scaler.fit_transform(test_data)</span><br><span class="line">    kmeans_kwargs = &#123;<span class="string">&quot;init&quot;</span>: <span class="string">&quot;random&quot;</span>, <span class="string">&quot;n_init&quot;</span>: <span class="number">5</span>, <span class="string">&quot;max_iter&quot;</span>: <span class="number">300</span>, <span class="string">&quot;random_state&quot;</span>: <span class="number">5726</span>&#125;</span><br><span class="line">    sse = []  <span class="comment"># A list holds the SSE values for each k</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)</span><br><span class="line">        kmeans.fit(scaled_features)</span><br><span class="line">        sse.append(kmeans.inertia_)</span><br><span class="line">    kl = KneeLocator(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>), sse, curve=<span class="string">&quot;convex&quot;</span>, direction=<span class="string">&quot;decreasing&quot;</span>)</span><br><span class="line">    kmeans = KMeans(init=<span class="string">&quot;random&quot;</span>, n_clusters=<span class="number">4</span>, n_init=<span class="number">5</span>, random_state=<span class="number">5726</span>)</span><br><span class="line">    kmeans.fit(scaled_features)</span><br><span class="line">    result = kmeans.predict(scaled_features2)</span><br><span class="line">    k = kl.elbow</span><br><span class="line">    model = kmeans</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model, k, result</span><br></pre></td></tr></table></figure>

<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">problem_2</span>(<span class="params">filename,predictors,target</span>):</span><br><span class="line">    <span class="comment"># write your logic here, model is the NN model</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup_seed</span>(<span class="params">seed</span>):</span><br><span class="line">        torch.manual_seed(seed)</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br><span class="line">        np.random.seed(seed)</span><br><span class="line">        random.seed(seed)</span><br><span class="line">        torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    setup_seed(<span class="number">5726</span>)</span><br><span class="line">    df = pd.read_csv(filename)</span><br><span class="line">    train_data = df[predictors]</span><br><span class="line">    label = df[target]</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(train_data, label, test_size=<span class="number">0.3</span>,</span><br><span class="line">                                                        random_state=<span class="number">5726</span>)</span><br><span class="line">    PredictorScaler = StandardScaler()</span><br><span class="line">    PredictorScalerFit = PredictorScaler.fit(X_train)</span><br><span class="line">    x_train = PredictorScalerFit.transform(X_train)</span><br><span class="line">    PredictorScaler = StandardScaler()</span><br><span class="line">    PredictorScalerFit = PredictorScaler.fit(X_test)</span><br><span class="line">    x_test = PredictorScalerFit.transform(X_test)</span><br><span class="line">    n_input, n_hidden1, n_hidden2, n_out, learning_rate = <span class="number">11</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">0.01</span></span><br><span class="line">    data_x = torch.FloatTensor(x_train)</span><br><span class="line">    y_train = np.array(y_train)</span><br><span class="line">    data_y = torch.FloatTensor(y_train).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">    model = nn.Sequential(nn.Linear(n_input, n_hidden1), nn.ReLU(),</span><br><span class="line">                          nn.Linear(n_hidden1, n_hidden2), nn.ReLU(),</span><br><span class="line">                          nn.Linear(n_hidden2, n_out), nn.Sigmoid())</span><br><span class="line">    loss_function = nn.MSELoss()</span><br><span class="line">    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line">    losses = []  <span class="comment"># Training Loop</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">        pred_y = model(data_x)</span><br><span class="line">        loss = loss_function(pred_y, data_y)</span><br><span class="line">        losses.append(loss.item())</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    targets = model(torch.FloatTensor(x_test))</span><br><span class="line">    test_recall = metrics.precision_score(np.<span class="built_in">round</span>(targets.detach().numpy()), y_test)</span><br><span class="line">    test_precision = metrics.recall_score(np.<span class="built_in">round</span>(targets.detach().numpy()), y_test)</span><br><span class="line">    model = model._modules</span><br><span class="line">    <span class="keyword">return</span> model, test_precision, test_recall</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>/2022/10/29/%E6%A0%91%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>逻辑回归</title>
    <url>/2022/07/27/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h1 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h1><p>对于二分类问题，我们期望通过监督学习，输出结果达到二分类的效果，如0代表负向分类，1代表正向分类。因此，所选择的函数的输出范围需要在[0,1]内，而对于线性回归模型，它的输出是离散的，因此我们要把离散的输出限制在连续范围[0,1]内。</p>
<p>有如下函数<br>$$<br>h(x)&#x3D;g(\theta^{T}x)&#x3D;\frac{1}{1+e^{-\theta^{T}x}}<br>$$</p>
<p>即<br>$$<br>g(z)&#x3D;\frac{1}{1+e^{-z}}<br>$$<br>其中$\theta^Tx$是线性函数，范围为$[-\infty,\infty]$，$g(z)$将$[-\infty,\infty]$的范围映射到$[0,1]$，通过这种方法，我门可以采用类似回归的方法解决二分类问题，$g(z)$的值被视为分到某一类的概率，即分到1或0类的概率为<br>$$<br>P(y&#x3D;1|x;\theta)&#x3D;h(x)&#x3D;g(\theta^{T}x)\<br>P(y&#x3D;0|x;\theta)&#x3D;1-h(x)&#x3D;1-g(\theta^{T}x)<br>$$<br>将上式可以更简洁地表示为<br>$$<br>P(y|x;\theta)&#x3D;（h(x))^y(1-h(x))^{1-y}<br>$$</p>
<h1 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h1><p>样本$(x_i,y_i)$满足独立同分布，因此，采用极大似然估计法估计参数$\theta$，首先写出似然函数<br>$$<br>\begin{aligned}<br>{ \max_{\theta}L(\theta)}<br>&amp; &#x3D; \prod_{i&#x3D;1}^mp(y^{(i)}|x^{(i)};\theta)\<br>&amp; &#x3D; \prod_{i&#x3D;1}^m(h(x^{(i)}))^{y^{(i)}}(1-h(x^{(i)}))^{1-y^{(i)}}<br>\end{aligned}<br>$$<br>对似然函数取对数，转换为求和<br>$$<br>l(\theta)&#x3D;logL(\theta)\<br>&#x3D;\sum_{i&#x3D;1}^my^{(i)}logh(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))<br>$$</p>
<h1 id="梯度上升法求解参数"><a href="#梯度上升法求解参数" class="headerlink" title="梯度上升法求解参数"></a>梯度上升法求解参数</h1><p>与线性回归中估计参数$\theta$的梯度下降方法类似，采用梯度上升方法，来最大化似然函数<br>$$<br>\theta:&#x3D;\theta+\alpha\nabla_{\theta}l(\theta)<br>$$</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial l(\theta)}{\partial\theta_j}<br>&amp; &#x3D; (y\frac{1}{g(\theta^{T}x)}-(1-y)\frac{1}{1-g(\theta^{T}x)})\frac{\partial g(\theta^{T}x)}{\partial\theta_j} \<br>&amp; &#x3D; (y\frac{1}{g(\theta^{T}x)}-(1-y)\frac{1}{1-g(\theta^{T}x)}) g(\theta^{T}x)(1-g(\theta^{T}x))\frac{\partial \theta^{T}x}{\partial\theta_j}\<br>&amp; &#x3D; (y(1-g(\theta^{T}x))-(1-y)g(\theta^{T}x)) x_j\<br>&amp; &#x3D; (y-h_{\theta}(x))x_j<br>\end{aligned}<br>$$</p>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>逻辑回归的损失函数即是在似然函数前加负号，即将极大化的似然函数转换为求最小值的损失函数。该损失函数又称为二进制交叉熵损失函数（BCE）<br>$$<br>H(\theta)&#x3D;-l(\theta)<br>&#x3D;-[\sum_{i&#x3D;1}^my^{(i)}logh(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))]<br>$$</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
</search>
